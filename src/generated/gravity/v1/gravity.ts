// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.7.0
//   protoc               v5.29.2
// source: gravity/v1/gravity.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import {
  type CallOptions,
  ChannelCredentials,
  Client,
  type ClientOptions,
  type ClientUnaryCall,
  type handleUnaryCall,
  makeGenericClientConstructor,
  Metadata,
  type ServiceError,
  type UntypedServiceImplementation,
} from "@grpc/grpc-js";
import { Empty } from "../../google/protobuf/empty";
import { Timestamp } from "../../google/protobuf/timestamp";

export const protobufPackage = "gravity.v1";

/** PublishDatasetRequest is the request message for publishing a dataset */
export interface PublishDatasetRequest {
  /** dataset_id: the ID of the dataset */
  datasetId: string;
}

/**
 * Crawler is a single crawler workflow that registers a single job
 * (platform/topic) on SN13's dynamic desirability engine
 */
export interface Crawler {
  /** crawler_id: the ID of the crawler */
  crawlerId: string;
  /** criteria: the contents of the job and the notification details */
  criteria?: CrawlerCriteria | undefined;
  /** start_time: the time the crawler was created */
  startTime?: Date | undefined;
  /** deregistration_time: the time the crawler was deregistered */
  deregistrationTime?: Date | undefined;
  /** archive_time: the time the crawler was archived */
  archiveTime?: Date | undefined;
  /** state: the current state of the crawler */
  state?: CrawlerState | undefined;
  /**
   * dataset_workflows: the IDs of the dataset workflows that are associated
   * with the crawler
   */
  datasetWorkflows: string[];
}

/** UpsertCrawlerRequest for upserting a crawler and its criteria */
export interface UpsertCrawlerRequest {
  /** gravity_task_id: the parent workflow id -- in this case the multicrawler id */
  gravityTaskId: string;
  /** crawler: the crawler to upsert into the database */
  crawler?: Crawler | undefined;
}

/** UpsertResponse is the response message for upserting a crawler */
export interface UpsertResponse {
  /**
   * message: the message of upserting a crawler (currently hardcoded to
   * "success")
   */
  message: string;
}

/** UpsertGravityTaskRequest for upserting a gravity task */
export interface UpsertGravityTaskRequest {
  /** gravity_task: the gravity task to upsert into the database */
  gravityTask?: GravityTaskRequest | undefined;
}

/**
 * UpsertGravityTaskResponse is the response message for upserting a gravity
 * task
 */
export interface UpsertGravityTaskResponse {
  /**
   * message: the message of upserting a gravity task (currently hardcoded to
   * "success")
   */
  message: string;
}

/** GravityTaskRequest represents the data needed to upsert a gravity task */
export interface GravityTaskRequest {
  /** id: the ID of the gravity task */
  id: string;
  /** name: the name of the gravity task */
  name: string;
  /** status: the status of the gravity task */
  status: string;
  /** start_time: the start time of the gravity task */
  startTime?: Date | undefined;
  /** notification_to: the notification email address */
  notificationTo: string;
  /** notification_link: the notification redirect link */
  notificationLink: string;
}

/** UpsertCrawlerCriteriaRequest for upserting a crawler and its criteria */
export interface InsertCrawlerCriteriaRequest {
  /** crawler_id: the id of the crawler */
  crawlerId: string;
  /** crawler_criteria: the crawler criteria to upsert into the database */
  crawlerCriteria?: CrawlerCriteria | undefined;
}

/** CrawlerCriteria is the contents of the job and the notification details */
export interface CrawlerCriteria {
  /** platform: the platform of the job ('x' or 'reddit') */
  platform: string;
  /** topic: the topic of the job (e.g. '#ai' for X, 'r/ai' for Reddit) */
  topic?: string | undefined;
  /** notification: the details of the notification to be sent to the user */
  notification?: CrawlerNotification | undefined;
  /** mock: Used for testing purposes (optional, defaults to false) */
  mock: boolean;
  /** user_id: the ID of the user who created the gravity task */
  userId: string;
  /** keyword: the keyword to search for in the job (optional) */
  keyword?: string | undefined;
  /** post_start_datetime: the start date of the job (optional) */
  postStartDatetime?: Date | undefined;
  /** post_end_datetime: the end date of the job (optional) */
  postEndDatetime?: Date | undefined;
}

/** CrawlerNotification is the details of the notification to be sent to the user */
export interface CrawlerNotification {
  /** to: the email address of the user */
  to: string;
  /** link: the redirect link in the email where the user can view the dataset */
  link: string;
}

/** HfRepo is a single Hugging Face repository that contains data for a crawler */
export interface HfRepo {
  /** repo_name: the name of the Hugging Face repository */
  repoName: string;
  /** row_count: the number of rows in the repository for the crawler criteria */
  rowCount: number;
  /** last_update: the last recorded time the repository was updated */
  lastUpdate: string;
}

/** CrawlerState is the current state of the crawler */
export interface CrawlerState {
  /**
   * status: the current status of the crawler
   *   "Pending"   -- Crawler is pending submission to the SN13 Validator
   *   "Submitted" -- Crawler is submitted to the SN13 Validator
   *   "Running"   -- Crawler is running (we got the first update)
   *   "Completed" -- Crawler is completed (timer expired)
   *   "Cancelled" -- Crawler is cancelled by user via cancellation of workflow
   *   "Archived"  -- Crawler is archived (now read-only i.e. no new dataset)
   *   "Failed"    -- Crawler failed to run
   */
  status: string;
  /** bytes_collected: the estimated number of bytes collected by the crawler */
  bytesCollected: number;
  /** records_collected: the estimated number of records collected by the crawler */
  recordsCollected: number;
  /** repos: the Hugging Face repositories that contain data for a crawler */
  repos: HfRepo[];
}

/** GravityTaskState is the current state of a gravity task */
export interface GravityTaskState {
  /** gravity_task_id: the ID of the gravity task */
  gravityTaskId: string;
  /** name: the name given by the user of the gravity task */
  name: string;
  /** status: the current status of the gravity task */
  status: string;
  /** start_time: the time the gravity task was created */
  startTime?: Date | undefined;
  /**
   * crawler_ids: the IDs of the crawler workflows that are associated with the
   * gravity task
   */
  crawlerIds: string[];
  /**
   * crawler_workflows: the crawler workflows that are associated with the
   * gravity task
   */
  crawlerWorkflows: Crawler[];
}

/**
 * GetGravityTasksRequest is the request message for listing gravity tasks for a
 * user
 */
export interface GetGravityTasksRequest {
  /**
   * gravity_task_id: the ID of the gravity task (optional, if not provided, all
   * gravity tasks for the user will be returned)
   */
  gravityTaskId?: string | undefined;
  /** include_crawlers: whether to include the crawler states in the response */
  includeCrawlers?: boolean | undefined;
}

/**
 * GetGravityTasksResponse is the response message for listing gravity tasks for
 * a user
 */
export interface GetGravityTasksResponse {
  /** gravity_task_states: the current states of the gravity tasks */
  gravityTaskStates: GravityTaskState[];
}

/** GravityTask defines a crawler's criteria for a single job (platform/topic) */
export interface GravityTask {
  /** topic: the topic of the job (e.g. '#ai' for X, 'r/ai' for Reddit) */
  topic?: string | undefined;
  /** platform: the platform of the job ('x' or 'reddit') */
  platform: string;
  /** keyword: the keyword to search for in the job (optional) */
  keyword?: string | undefined;
  /** post_start_datetime: the start date of the job (optional) */
  postStartDatetime?: Date | undefined;
  /** post_end_datetime: the end date of the job (optional) */
  postEndDatetime?: Date | undefined;
}

/**
 * NotificationRequest is the request message for sending a notification to a
 * user when a dataset is ready to download
 */
export interface NotificationRequest {
  /**
   * type: the type of notification to send ('email' is only supported
   * currently)
   */
  type: string;
  /**
   * address: the address to send the notification to (only email addresses are
   * supported currently)
   */
  address: string;
  /**
   * redirect_url: the URL to include in the notication message that redirects
   * the user to any built datasets
   */
  redirectUrl?: string | undefined;
}

/** GetCrawlerRequest is the request message for getting a crawler */
export interface GetCrawlerRequest {
  /** crawler_id: the ID of the crawler */
  crawlerId: string;
}

/** GetCrawlerResponse is the response message for getting a crawler */
export interface GetCrawlerResponse {
  /** crawler: the crawler */
  crawler?: Crawler | undefined;
}

/**
 * CreateGravityTaskRequest is the request message for creating a new gravity
 * task
 */
export interface CreateGravityTaskRequest {
  /** gravity_tasks: the criteria for the crawlers that will be created */
  gravityTasks: GravityTask[];
  /**
   * name: the name of the gravity task (optional, default will generate a
   * random name)
   */
  name: string;
  /**
   * notification_requests: the details of the notification to be sent to the
   * user when a dataset
   *   that is automatically generated upon completion of the crawler is ready
   *   to download (optional)
   */
  notificationRequests: NotificationRequest[];
  /**
   * gravity_task_id: the ID of the gravity task (optional, default will
   * generate a random ID)
   */
  gravityTaskId?: string | undefined;
}

/**
 * CreateGravityTaskResponse is the response message for creating a new gravity
 * task
 */
export interface CreateGravityTaskResponse {
  /** gravity_task_id: the ID of the gravity task */
  gravityTaskId: string;
}

/**
 * BuildDatasetRequest is the request message for manually requesting the
 * building of a dataset for a single crawler
 */
export interface BuildDatasetRequest {
  /** crawler_id: the ID of the crawler that will be used to build the dataset */
  crawlerId: string;
  /**
   * notification_requests: the details of the notification to be sent to the
   * user when the dataset is ready to download (optional)
   */
  notificationRequests: NotificationRequest[];
  /**
   * max_rows: the maximum number of rows to include in the dataset (optional,
   * defaults to 500)
   */
  maxRows: number;
}

/**
 * BuildDatasetResponse is the response message for manually requesting the
 * building of a dataset for a single crawler
 * - dataset: the dataset that was built
 */
export interface BuildDatasetResponse {
  /** dataset_id: the ID of the dataset */
  datasetId: string;
  /** dataset: the dataset that was built */
  dataset?: Dataset | undefined;
}

/**
 * BuildAllDatasetsRequest is the request message for building all datasets
 * belonging to a workflow
 */
export interface BuildAllDatasetsRequest {
  /** gravityTaskId specifies which task to build */
  gravityTaskId: string;
  /** specifies how much of each crawler to build for workflow */
  buildCrawlersConfig: BuildDatasetRequest[];
}

export interface BuildAllDatasetsResponse {
  gravityTaskId: string;
  datasets: Dataset[];
}

export interface Nebula {
  /** error: nebula build error message */
  error: string;
  /** file_size_bytes: the size of the file in bytes */
  fileSizeBytes: number;
  /** url: the URL of the file */
  url: string;
}

/** Dataset contains the progress and results of a dataset build */
export interface Dataset {
  /** crawler_workflow_id: the ID of the parent crawler for this dataset */
  crawlerWorkflowId: string;
  /** create_date: the date the dataset was created */
  createDate?: Date | undefined;
  /** expire_date: the date the dataset will expire (be deleted) */
  expireDate?: Date | undefined;
  /** files: the details about the dataset files that are included in the dataset */
  files: DatasetFile[];
  /** status: the status of the dataset */
  status: string;
  /** status_message: the message of the status of the dataset */
  statusMessage: string;
  /** steps: the progress of the dataset build */
  steps: DatasetStep[];
  /** total_steps: the total number of steps in the dataset build */
  totalSteps: number;
  /** nebula: the details about the nebula that was built */
  nebula?: Nebula | undefined;
}

/**
 * UpsertDatasetRequest contains the dataset id to insert and the dataset
 * details
 */
export interface UpsertDatasetRequest {
  /** dataset_id: a unique id for the dataset */
  datasetId: string;
  /** dataset: the details of the dataset */
  dataset?: Dataset | undefined;
}

/** UpsertNebulaRequest contains the dataset id and nebula details to upsert */
export interface UpsertNebulaRequest {
  /** dataset_id: a unique id for the dataset */
  datasetId: string;
  /** nebula_id: a unique id for the nebula */
  nebulaId: string;
  /** nebula: the details of the nebula */
  nebula?: Nebula | undefined;
}

/**
 * InsertDatasetFileRequest contains the dataset id to insert into and the
 * dataset file details
 */
export interface InsertDatasetFileRequest {
  /** dataset_id: the ID of the dataset to attach the file to */
  datasetId: string;
  /** files: the dataset files to insert */
  files: DatasetFile[];
}

/** DatasetFile contains the details about a dataset file */
export interface DatasetFile {
  /** file_name: the name of the file */
  fileName: string;
  /** file_size_bytes: the size of the file in bytes */
  fileSizeBytes: number;
  /** last_modified: the date the file was last modified */
  lastModified?: Date | undefined;
  /** num_rows: the number of rows in the file */
  numRows: number;
  /** s3_key: the key of the file in S3 (internal use only) */
  s3Key: string;
  /** url: the URL of the file (public use) */
  url: string;
}

/**
 * DatasetStep contains one step of the progress of a dataset build
 * (NOTE: each step varies in time and complexity)
 */
export interface DatasetStep {
  /** progress: the progress of this step in the dataset build (0.0 - 1.0) */
  progress: number;
  /** step: the step number of the dataset build (1-indexed) */
  step: number;
  /** step_name: description of what is happening in the step */
  stepName: string;
}

/** GetDatasetRequest is the request message for getting the status of a dataset */
export interface GetDatasetRequest {
  /** dataset_id: the ID of the dataset */
  datasetId: string;
}

/**
 * GetDatasetResponse is the response message for getting the status of a
 * dataset
 */
export interface GetDatasetResponse {
  /** dataset: the dataset that is being built */
  dataset?: Dataset | undefined;
}

/** CancelGravityTaskRequest is the request message for cancelling a gravity task */
export interface CancelGravityTaskRequest {
  /** gravity_task_id: the ID of the gravity task */
  gravityTaskId: string;
}

/**
 * CancelGravityTaskResponse is the response message for cancelling a gravity
 * task
 */
export interface CancelGravityTaskResponse {
  /**
   * message: the message of the cancellation of the gravity task (currently
   * hardcoded to "success")
   */
  message: string;
}

/** CancelDatasetRequest is the request message for cancelling a dataset build */
export interface CancelDatasetRequest {
  /** dataset_id: the ID of the dataset */
  datasetId: string;
}

/** CancelDatasetResponse is the response message for cancelling a dataset build */
export interface CancelDatasetResponse {
  /**
   * message: the message of the cancellation of the dataset build (currently
   * hardcoded to "success")
   */
  message: string;
}

/** DatasetBillingCorrectionRequest is the request message for refunding a user */
export interface DatasetBillingCorrectionRequest {
  /** requested_row_count: number of rows expected by the user */
  requestedRowCount: number;
  /** actual_row_count: number of rows returned by gravity */
  actualRowCount: number;
}

/** DatasetBillingCorrectionResponse is the response message for refunding a user */
export interface DatasetBillingCorrectionResponse {
  /** refund_amount */
  refundAmount: number;
}

/**
 * GetMarketplaceDatasetsResponse returns the dataset metadata to be used in
 * Marketplace
 */
export interface GetMarketplaceDatasetsResponse {
  /** datasets: list of marketplace datasets */
  datasets: DatasetFile[];
}

/**
 * GetGravityTaskDatasetFilesRequest is the request message for getting dataset
 * files for a gravity task
 */
export interface GetGravityTaskDatasetFilesRequest {
  /** gravity_task_id: the ID of the gravity task (required) */
  gravityTaskId: string;
}

/** CrawlerDatasetFiles contains dataset files for a specific crawler */
export interface CrawlerDatasetFiles {
  /** crawler_id: the ID of the crawler */
  crawlerId: string;
  /** dataset_files: the dataset files associated with this crawler */
  datasetFiles: DatasetFileWithId[];
}

/** DatasetFileWithId extends DatasetFile to include the dataset ID */
export interface DatasetFileWithId {
  /** dataset_id: the ID of the dataset this file belongs to */
  datasetId: string;
  /** file_name: the name of the file */
  fileName: string;
  /** file_size_bytes: the size of the file in bytes */
  fileSizeBytes: number;
  /** last_modified: the date the file was last modified */
  lastModified?: Date | undefined;
  /** num_rows: the number of rows in the file */
  numRows: number;
  /** s3_key: the key of the file in S3 (internal use only) */
  s3Key: string;
  /** url: the URL of the file (public use) */
  url: string;
}

/**
 * GetGravityTaskDatasetFilesResponse is the response message for getting
 * dataset files for a gravity task
 */
export interface GetGravityTaskDatasetFilesResponse {
  /** gravity_task_id: the ID of the gravity task */
  gravityTaskId: string;
  /** crawler_dataset_files: dataset files grouped by crawler */
  crawlerDatasetFiles: CrawlerDatasetFiles[];
}

function createBasePublishDatasetRequest(): PublishDatasetRequest {
  return { datasetId: "" };
}

export const PublishDatasetRequest: MessageFns<PublishDatasetRequest> = {
  encode(
    message: PublishDatasetRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): PublishDatasetRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBasePublishDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): PublishDatasetRequest {
    return {
      datasetId: isSet(object.datasetId)
        ? globalThis.String(object.datasetId)
        : "",
    };
  },

  toJSON(message: PublishDatasetRequest): unknown {
    const obj: any = {};
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    return obj;
  },

  create(base?: DeepPartial<PublishDatasetRequest>): PublishDatasetRequest {
    return PublishDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<PublishDatasetRequest>,
  ): PublishDatasetRequest {
    const message = createBasePublishDatasetRequest();
    message.datasetId = object.datasetId ?? "";
    return message;
  },
};

function createBaseCrawler(): Crawler {
  return {
    crawlerId: "",
    criteria: undefined,
    startTime: undefined,
    deregistrationTime: undefined,
    archiveTime: undefined,
    state: undefined,
    datasetWorkflows: [],
  };
}

export const Crawler: MessageFns<Crawler> = {
  encode(
    message: Crawler,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.crawlerId !== "") {
      writer.uint32(10).string(message.crawlerId);
    }
    if (message.criteria !== undefined) {
      CrawlerCriteria.encode(message.criteria, writer.uint32(18).fork()).join();
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(
        toTimestamp(message.startTime),
        writer.uint32(26).fork(),
      ).join();
    }
    if (message.deregistrationTime !== undefined) {
      Timestamp.encode(
        toTimestamp(message.deregistrationTime),
        writer.uint32(34).fork(),
      ).join();
    }
    if (message.archiveTime !== undefined) {
      Timestamp.encode(
        toTimestamp(message.archiveTime),
        writer.uint32(42).fork(),
      ).join();
    }
    if (message.state !== undefined) {
      CrawlerState.encode(message.state, writer.uint32(50).fork()).join();
    }
    for (const v of message.datasetWorkflows) {
      writer.uint32(58).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Crawler {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCrawler();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.crawlerId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.criteria = CrawlerCriteria.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.startTime = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.deregistrationTime = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.archiveTime = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.state = CrawlerState.decode(reader, reader.uint32());
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.datasetWorkflows.push(reader.string());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Crawler {
    return {
      crawlerId: isSet(object.crawlerId)
        ? globalThis.String(object.crawlerId)
        : "",
      criteria: isSet(object.criteria)
        ? CrawlerCriteria.fromJSON(object.criteria)
        : undefined,
      startTime: isSet(object.startTime)
        ? fromJsonTimestamp(object.startTime)
        : undefined,
      deregistrationTime: isSet(object.deregistrationTime)
        ? fromJsonTimestamp(object.deregistrationTime)
        : undefined,
      archiveTime: isSet(object.archiveTime)
        ? fromJsonTimestamp(object.archiveTime)
        : undefined,
      state: isSet(object.state)
        ? CrawlerState.fromJSON(object.state)
        : undefined,
      datasetWorkflows: globalThis.Array.isArray(object?.datasetWorkflows)
        ? object.datasetWorkflows.map((e: any) => globalThis.String(e))
        : [],
    };
  },

  toJSON(message: Crawler): unknown {
    const obj: any = {};
    if (message.crawlerId !== "") {
      obj.crawlerId = message.crawlerId;
    }
    if (message.criteria !== undefined) {
      obj.criteria = CrawlerCriteria.toJSON(message.criteria);
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.deregistrationTime !== undefined) {
      obj.deregistrationTime = message.deregistrationTime.toISOString();
    }
    if (message.archiveTime !== undefined) {
      obj.archiveTime = message.archiveTime.toISOString();
    }
    if (message.state !== undefined) {
      obj.state = CrawlerState.toJSON(message.state);
    }
    if (message.datasetWorkflows?.length) {
      obj.datasetWorkflows = message.datasetWorkflows;
    }
    return obj;
  },

  create(base?: DeepPartial<Crawler>): Crawler {
    return Crawler.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Crawler>): Crawler {
    const message = createBaseCrawler();
    message.crawlerId = object.crawlerId ?? "";
    message.criteria =
      object.criteria !== undefined && object.criteria !== null
        ? CrawlerCriteria.fromPartial(object.criteria)
        : undefined;
    message.startTime = object.startTime ?? undefined;
    message.deregistrationTime = object.deregistrationTime ?? undefined;
    message.archiveTime = object.archiveTime ?? undefined;
    message.state =
      object.state !== undefined && object.state !== null
        ? CrawlerState.fromPartial(object.state)
        : undefined;
    message.datasetWorkflows = object.datasetWorkflows?.map(e => e) || [];
    return message;
  },
};

function createBaseUpsertCrawlerRequest(): UpsertCrawlerRequest {
  return { gravityTaskId: "", crawler: undefined };
}

export const UpsertCrawlerRequest: MessageFns<UpsertCrawlerRequest> = {
  encode(
    message: UpsertCrawlerRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.gravityTaskId !== "") {
      writer.uint32(10).string(message.gravityTaskId);
    }
    if (message.crawler !== undefined) {
      Crawler.encode(message.crawler, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): UpsertCrawlerRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpsertCrawlerRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.gravityTaskId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.crawler = Crawler.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpsertCrawlerRequest {
    return {
      gravityTaskId: isSet(object.gravityTaskId)
        ? globalThis.String(object.gravityTaskId)
        : "",
      crawler: isSet(object.crawler)
        ? Crawler.fromJSON(object.crawler)
        : undefined,
    };
  },

  toJSON(message: UpsertCrawlerRequest): unknown {
    const obj: any = {};
    if (message.gravityTaskId !== "") {
      obj.gravityTaskId = message.gravityTaskId;
    }
    if (message.crawler !== undefined) {
      obj.crawler = Crawler.toJSON(message.crawler);
    }
    return obj;
  },

  create(base?: DeepPartial<UpsertCrawlerRequest>): UpsertCrawlerRequest {
    return UpsertCrawlerRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpsertCrawlerRequest>): UpsertCrawlerRequest {
    const message = createBaseUpsertCrawlerRequest();
    message.gravityTaskId = object.gravityTaskId ?? "";
    message.crawler =
      object.crawler !== undefined && object.crawler !== null
        ? Crawler.fromPartial(object.crawler)
        : undefined;
    return message;
  },
};

function createBaseUpsertResponse(): UpsertResponse {
  return { message: "" };
}

export const UpsertResponse: MessageFns<UpsertResponse> = {
  encode(
    message: UpsertResponse,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.message !== "") {
      writer.uint32(10).string(message.message);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): UpsertResponse {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpsertResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.message = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpsertResponse {
    return {
      message: isSet(object.message) ? globalThis.String(object.message) : "",
    };
  },

  toJSON(message: UpsertResponse): unknown {
    const obj: any = {};
    if (message.message !== "") {
      obj.message = message.message;
    }
    return obj;
  },

  create(base?: DeepPartial<UpsertResponse>): UpsertResponse {
    return UpsertResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpsertResponse>): UpsertResponse {
    const message = createBaseUpsertResponse();
    message.message = object.message ?? "";
    return message;
  },
};

function createBaseUpsertGravityTaskRequest(): UpsertGravityTaskRequest {
  return { gravityTask: undefined };
}

export const UpsertGravityTaskRequest: MessageFns<UpsertGravityTaskRequest> = {
  encode(
    message: UpsertGravityTaskRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.gravityTask !== undefined) {
      GravityTaskRequest.encode(
        message.gravityTask,
        writer.uint32(10).fork(),
      ).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): UpsertGravityTaskRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpsertGravityTaskRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.gravityTask = GravityTaskRequest.decode(
            reader,
            reader.uint32(),
          );
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpsertGravityTaskRequest {
    return {
      gravityTask: isSet(object.gravityTask)
        ? GravityTaskRequest.fromJSON(object.gravityTask)
        : undefined,
    };
  },

  toJSON(message: UpsertGravityTaskRequest): unknown {
    const obj: any = {};
    if (message.gravityTask !== undefined) {
      obj.gravityTask = GravityTaskRequest.toJSON(message.gravityTask);
    }
    return obj;
  },

  create(
    base?: DeepPartial<UpsertGravityTaskRequest>,
  ): UpsertGravityTaskRequest {
    return UpsertGravityTaskRequest.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<UpsertGravityTaskRequest>,
  ): UpsertGravityTaskRequest {
    const message = createBaseUpsertGravityTaskRequest();
    message.gravityTask =
      object.gravityTask !== undefined && object.gravityTask !== null
        ? GravityTaskRequest.fromPartial(object.gravityTask)
        : undefined;
    return message;
  },
};

function createBaseUpsertGravityTaskResponse(): UpsertGravityTaskResponse {
  return { message: "" };
}

export const UpsertGravityTaskResponse: MessageFns<UpsertGravityTaskResponse> =
  {
    encode(
      message: UpsertGravityTaskResponse,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.message !== "") {
        writer.uint32(10).string(message.message);
      }
      return writer;
    },

    decode(
      input: BinaryReader | Uint8Array,
      length?: number,
    ): UpsertGravityTaskResponse {
      const reader =
        input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseUpsertGravityTaskResponse();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1: {
            if (tag !== 10) {
              break;
            }

            message.message = reader.string();
            continue;
          }
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): UpsertGravityTaskResponse {
      return {
        message: isSet(object.message) ? globalThis.String(object.message) : "",
      };
    },

    toJSON(message: UpsertGravityTaskResponse): unknown {
      const obj: any = {};
      if (message.message !== "") {
        obj.message = message.message;
      }
      return obj;
    },

    create(
      base?: DeepPartial<UpsertGravityTaskResponse>,
    ): UpsertGravityTaskResponse {
      return UpsertGravityTaskResponse.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<UpsertGravityTaskResponse>,
    ): UpsertGravityTaskResponse {
      const message = createBaseUpsertGravityTaskResponse();
      message.message = object.message ?? "";
      return message;
    },
  };

function createBaseGravityTaskRequest(): GravityTaskRequest {
  return {
    id: "",
    name: "",
    status: "",
    startTime: undefined,
    notificationTo: "",
    notificationLink: "",
  };
}

export const GravityTaskRequest: MessageFns<GravityTaskRequest> = {
  encode(
    message: GravityTaskRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.id !== "") {
      writer.uint32(10).string(message.id);
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.status !== "") {
      writer.uint32(26).string(message.status);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(
        toTimestamp(message.startTime),
        writer.uint32(34).fork(),
      ).join();
    }
    if (message.notificationTo !== "") {
      writer.uint32(42).string(message.notificationTo);
    }
    if (message.notificationLink !== "") {
      writer.uint32(50).string(message.notificationLink);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): GravityTaskRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGravityTaskRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.id = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.status = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.startTime = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.notificationTo = reader.string();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.notificationLink = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GravityTaskRequest {
    return {
      id: isSet(object.id) ? globalThis.String(object.id) : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      status: isSet(object.status) ? globalThis.String(object.status) : "",
      startTime: isSet(object.startTime)
        ? fromJsonTimestamp(object.startTime)
        : undefined,
      notificationTo: isSet(object.notificationTo)
        ? globalThis.String(object.notificationTo)
        : "",
      notificationLink: isSet(object.notificationLink)
        ? globalThis.String(object.notificationLink)
        : "",
    };
  },

  toJSON(message: GravityTaskRequest): unknown {
    const obj: any = {};
    if (message.id !== "") {
      obj.id = message.id;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.status !== "") {
      obj.status = message.status;
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.notificationTo !== "") {
      obj.notificationTo = message.notificationTo;
    }
    if (message.notificationLink !== "") {
      obj.notificationLink = message.notificationLink;
    }
    return obj;
  },

  create(base?: DeepPartial<GravityTaskRequest>): GravityTaskRequest {
    return GravityTaskRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GravityTaskRequest>): GravityTaskRequest {
    const message = createBaseGravityTaskRequest();
    message.id = object.id ?? "";
    message.name = object.name ?? "";
    message.status = object.status ?? "";
    message.startTime = object.startTime ?? undefined;
    message.notificationTo = object.notificationTo ?? "";
    message.notificationLink = object.notificationLink ?? "";
    return message;
  },
};

function createBaseInsertCrawlerCriteriaRequest(): InsertCrawlerCriteriaRequest {
  return { crawlerId: "", crawlerCriteria: undefined };
}

export const InsertCrawlerCriteriaRequest: MessageFns<InsertCrawlerCriteriaRequest> =
  {
    encode(
      message: InsertCrawlerCriteriaRequest,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.crawlerId !== "") {
        writer.uint32(10).string(message.crawlerId);
      }
      if (message.crawlerCriteria !== undefined) {
        CrawlerCriteria.encode(
          message.crawlerCriteria,
          writer.uint32(18).fork(),
        ).join();
      }
      return writer;
    },

    decode(
      input: BinaryReader | Uint8Array,
      length?: number,
    ): InsertCrawlerCriteriaRequest {
      const reader =
        input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseInsertCrawlerCriteriaRequest();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1: {
            if (tag !== 10) {
              break;
            }

            message.crawlerId = reader.string();
            continue;
          }
          case 2: {
            if (tag !== 18) {
              break;
            }

            message.crawlerCriteria = CrawlerCriteria.decode(
              reader,
              reader.uint32(),
            );
            continue;
          }
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): InsertCrawlerCriteriaRequest {
      return {
        crawlerId: isSet(object.crawlerId)
          ? globalThis.String(object.crawlerId)
          : "",
        crawlerCriteria: isSet(object.crawlerCriteria)
          ? CrawlerCriteria.fromJSON(object.crawlerCriteria)
          : undefined,
      };
    },

    toJSON(message: InsertCrawlerCriteriaRequest): unknown {
      const obj: any = {};
      if (message.crawlerId !== "") {
        obj.crawlerId = message.crawlerId;
      }
      if (message.crawlerCriteria !== undefined) {
        obj.crawlerCriteria = CrawlerCriteria.toJSON(message.crawlerCriteria);
      }
      return obj;
    },

    create(
      base?: DeepPartial<InsertCrawlerCriteriaRequest>,
    ): InsertCrawlerCriteriaRequest {
      return InsertCrawlerCriteriaRequest.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<InsertCrawlerCriteriaRequest>,
    ): InsertCrawlerCriteriaRequest {
      const message = createBaseInsertCrawlerCriteriaRequest();
      message.crawlerId = object.crawlerId ?? "";
      message.crawlerCriteria =
        object.crawlerCriteria !== undefined && object.crawlerCriteria !== null
          ? CrawlerCriteria.fromPartial(object.crawlerCriteria)
          : undefined;
      return message;
    },
  };

function createBaseCrawlerCriteria(): CrawlerCriteria {
  return {
    platform: "",
    topic: undefined,
    notification: undefined,
    mock: false,
    userId: "",
    keyword: undefined,
    postStartDatetime: undefined,
    postEndDatetime: undefined,
  };
}

export const CrawlerCriteria: MessageFns<CrawlerCriteria> = {
  encode(
    message: CrawlerCriteria,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.platform !== "") {
      writer.uint32(10).string(message.platform);
    }
    if (message.topic !== undefined) {
      writer.uint32(18).string(message.topic);
    }
    if (message.notification !== undefined) {
      CrawlerNotification.encode(
        message.notification,
        writer.uint32(26).fork(),
      ).join();
    }
    if (message.mock !== false) {
      writer.uint32(32).bool(message.mock);
    }
    if (message.userId !== "") {
      writer.uint32(42).string(message.userId);
    }
    if (message.keyword !== undefined) {
      writer.uint32(50).string(message.keyword);
    }
    if (message.postStartDatetime !== undefined) {
      Timestamp.encode(
        toTimestamp(message.postStartDatetime),
        writer.uint32(58).fork(),
      ).join();
    }
    if (message.postEndDatetime !== undefined) {
      Timestamp.encode(
        toTimestamp(message.postEndDatetime),
        writer.uint32(66).fork(),
      ).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CrawlerCriteria {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCrawlerCriteria();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.platform = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.topic = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.notification = CrawlerNotification.decode(
            reader,
            reader.uint32(),
          );
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.mock = reader.bool();
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.userId = reader.string();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.keyword = reader.string();
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.postStartDatetime = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 8: {
          if (tag !== 66) {
            break;
          }

          message.postEndDatetime = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CrawlerCriteria {
    return {
      platform: isSet(object.platform)
        ? globalThis.String(object.platform)
        : "",
      topic: isSet(object.topic) ? globalThis.String(object.topic) : undefined,
      notification: isSet(object.notification)
        ? CrawlerNotification.fromJSON(object.notification)
        : undefined,
      mock: isSet(object.mock) ? globalThis.Boolean(object.mock) : false,
      userId: isSet(object.userId) ? globalThis.String(object.userId) : "",
      keyword: isSet(object.keyword)
        ? globalThis.String(object.keyword)
        : undefined,
      postStartDatetime: isSet(object.postStartDatetime)
        ? fromJsonTimestamp(object.postStartDatetime)
        : undefined,
      postEndDatetime: isSet(object.postEndDatetime)
        ? fromJsonTimestamp(object.postEndDatetime)
        : undefined,
    };
  },

  toJSON(message: CrawlerCriteria): unknown {
    const obj: any = {};
    if (message.platform !== "") {
      obj.platform = message.platform;
    }
    if (message.topic !== undefined) {
      obj.topic = message.topic;
    }
    if (message.notification !== undefined) {
      obj.notification = CrawlerNotification.toJSON(message.notification);
    }
    if (message.mock !== false) {
      obj.mock = message.mock;
    }
    if (message.userId !== "") {
      obj.userId = message.userId;
    }
    if (message.keyword !== undefined) {
      obj.keyword = message.keyword;
    }
    if (message.postStartDatetime !== undefined) {
      obj.postStartDatetime = message.postStartDatetime.toISOString();
    }
    if (message.postEndDatetime !== undefined) {
      obj.postEndDatetime = message.postEndDatetime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<CrawlerCriteria>): CrawlerCriteria {
    return CrawlerCriteria.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CrawlerCriteria>): CrawlerCriteria {
    const message = createBaseCrawlerCriteria();
    message.platform = object.platform ?? "";
    message.topic = object.topic ?? undefined;
    message.notification =
      object.notification !== undefined && object.notification !== null
        ? CrawlerNotification.fromPartial(object.notification)
        : undefined;
    message.mock = object.mock ?? false;
    message.userId = object.userId ?? "";
    message.keyword = object.keyword ?? undefined;
    message.postStartDatetime = object.postStartDatetime ?? undefined;
    message.postEndDatetime = object.postEndDatetime ?? undefined;
    return message;
  },
};

function createBaseCrawlerNotification(): CrawlerNotification {
  return { to: "", link: "" };
}

export const CrawlerNotification: MessageFns<CrawlerNotification> = {
  encode(
    message: CrawlerNotification,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.to !== "") {
      writer.uint32(10).string(message.to);
    }
    if (message.link !== "") {
      writer.uint32(18).string(message.link);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): CrawlerNotification {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCrawlerNotification();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.to = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.link = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CrawlerNotification {
    return {
      to: isSet(object.to) ? globalThis.String(object.to) : "",
      link: isSet(object.link) ? globalThis.String(object.link) : "",
    };
  },

  toJSON(message: CrawlerNotification): unknown {
    const obj: any = {};
    if (message.to !== "") {
      obj.to = message.to;
    }
    if (message.link !== "") {
      obj.link = message.link;
    }
    return obj;
  },

  create(base?: DeepPartial<CrawlerNotification>): CrawlerNotification {
    return CrawlerNotification.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CrawlerNotification>): CrawlerNotification {
    const message = createBaseCrawlerNotification();
    message.to = object.to ?? "";
    message.link = object.link ?? "";
    return message;
  },
};

function createBaseHfRepo(): HfRepo {
  return { repoName: "", rowCount: 0, lastUpdate: "" };
}

export const HfRepo: MessageFns<HfRepo> = {
  encode(
    message: HfRepo,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.repoName !== "") {
      writer.uint32(10).string(message.repoName);
    }
    if (message.rowCount !== 0) {
      writer.uint32(16).uint64(message.rowCount);
    }
    if (message.lastUpdate !== "") {
      writer.uint32(26).string(message.lastUpdate);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): HfRepo {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseHfRepo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.repoName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.rowCount = longToNumber(reader.uint64());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.lastUpdate = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): HfRepo {
    return {
      repoName: isSet(object.repoName)
        ? globalThis.String(object.repoName)
        : "",
      rowCount: isSet(object.rowCount) ? globalThis.Number(object.rowCount) : 0,
      lastUpdate: isSet(object.lastUpdate)
        ? globalThis.String(object.lastUpdate)
        : "",
    };
  },

  toJSON(message: HfRepo): unknown {
    const obj: any = {};
    if (message.repoName !== "") {
      obj.repoName = message.repoName;
    }
    if (message.rowCount !== 0) {
      obj.rowCount = Math.round(message.rowCount);
    }
    if (message.lastUpdate !== "") {
      obj.lastUpdate = message.lastUpdate;
    }
    return obj;
  },

  create(base?: DeepPartial<HfRepo>): HfRepo {
    return HfRepo.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<HfRepo>): HfRepo {
    const message = createBaseHfRepo();
    message.repoName = object.repoName ?? "";
    message.rowCount = object.rowCount ?? 0;
    message.lastUpdate = object.lastUpdate ?? "";
    return message;
  },
};

function createBaseCrawlerState(): CrawlerState {
  return { status: "", bytesCollected: 0, recordsCollected: 0, repos: [] };
}

export const CrawlerState: MessageFns<CrawlerState> = {
  encode(
    message: CrawlerState,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.status !== "") {
      writer.uint32(10).string(message.status);
    }
    if (message.bytesCollected !== 0) {
      writer.uint32(16).uint64(message.bytesCollected);
    }
    if (message.recordsCollected !== 0) {
      writer.uint32(24).uint64(message.recordsCollected);
    }
    for (const v of message.repos) {
      HfRepo.encode(v!, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): CrawlerState {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCrawlerState();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.status = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.bytesCollected = longToNumber(reader.uint64());
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.recordsCollected = longToNumber(reader.uint64());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.repos.push(HfRepo.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CrawlerState {
    return {
      status: isSet(object.status) ? globalThis.String(object.status) : "",
      bytesCollected: isSet(object.bytesCollected)
        ? globalThis.Number(object.bytesCollected)
        : 0,
      recordsCollected: isSet(object.recordsCollected)
        ? globalThis.Number(object.recordsCollected)
        : 0,
      repos: globalThis.Array.isArray(object?.repos)
        ? object.repos.map((e: any) => HfRepo.fromJSON(e))
        : [],
    };
  },

  toJSON(message: CrawlerState): unknown {
    const obj: any = {};
    if (message.status !== "") {
      obj.status = message.status;
    }
    if (message.bytesCollected !== 0) {
      obj.bytesCollected = Math.round(message.bytesCollected);
    }
    if (message.recordsCollected !== 0) {
      obj.recordsCollected = Math.round(message.recordsCollected);
    }
    if (message.repos?.length) {
      obj.repos = message.repos.map(e => HfRepo.toJSON(e));
    }
    return obj;
  },

  create(base?: DeepPartial<CrawlerState>): CrawlerState {
    return CrawlerState.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CrawlerState>): CrawlerState {
    const message = createBaseCrawlerState();
    message.status = object.status ?? "";
    message.bytesCollected = object.bytesCollected ?? 0;
    message.recordsCollected = object.recordsCollected ?? 0;
    message.repos = object.repos?.map(e => HfRepo.fromPartial(e)) || [];
    return message;
  },
};

function createBaseGravityTaskState(): GravityTaskState {
  return {
    gravityTaskId: "",
    name: "",
    status: "",
    startTime: undefined,
    crawlerIds: [],
    crawlerWorkflows: [],
  };
}

export const GravityTaskState: MessageFns<GravityTaskState> = {
  encode(
    message: GravityTaskState,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.gravityTaskId !== "") {
      writer.uint32(10).string(message.gravityTaskId);
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    if (message.status !== "") {
      writer.uint32(26).string(message.status);
    }
    if (message.startTime !== undefined) {
      Timestamp.encode(
        toTimestamp(message.startTime),
        writer.uint32(34).fork(),
      ).join();
    }
    for (const v of message.crawlerIds) {
      writer.uint32(42).string(v!);
    }
    for (const v of message.crawlerWorkflows) {
      Crawler.encode(v!, writer.uint32(50).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GravityTaskState {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGravityTaskState();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.gravityTaskId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.status = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.startTime = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.crawlerIds.push(reader.string());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.crawlerWorkflows.push(
            Crawler.decode(reader, reader.uint32()),
          );
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GravityTaskState {
    return {
      gravityTaskId: isSet(object.gravityTaskId)
        ? globalThis.String(object.gravityTaskId)
        : "",
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      status: isSet(object.status) ? globalThis.String(object.status) : "",
      startTime: isSet(object.startTime)
        ? fromJsonTimestamp(object.startTime)
        : undefined,
      crawlerIds: globalThis.Array.isArray(object?.crawlerIds)
        ? object.crawlerIds.map((e: any) => globalThis.String(e))
        : [],
      crawlerWorkflows: globalThis.Array.isArray(object?.crawlerWorkflows)
        ? object.crawlerWorkflows.map((e: any) => Crawler.fromJSON(e))
        : [],
    };
  },

  toJSON(message: GravityTaskState): unknown {
    const obj: any = {};
    if (message.gravityTaskId !== "") {
      obj.gravityTaskId = message.gravityTaskId;
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.status !== "") {
      obj.status = message.status;
    }
    if (message.startTime !== undefined) {
      obj.startTime = message.startTime.toISOString();
    }
    if (message.crawlerIds?.length) {
      obj.crawlerIds = message.crawlerIds;
    }
    if (message.crawlerWorkflows?.length) {
      obj.crawlerWorkflows = message.crawlerWorkflows.map(e =>
        Crawler.toJSON(e),
      );
    }
    return obj;
  },

  create(base?: DeepPartial<GravityTaskState>): GravityTaskState {
    return GravityTaskState.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GravityTaskState>): GravityTaskState {
    const message = createBaseGravityTaskState();
    message.gravityTaskId = object.gravityTaskId ?? "";
    message.name = object.name ?? "";
    message.status = object.status ?? "";
    message.startTime = object.startTime ?? undefined;
    message.crawlerIds = object.crawlerIds?.map(e => e) || [];
    message.crawlerWorkflows =
      object.crawlerWorkflows?.map(e => Crawler.fromPartial(e)) || [];
    return message;
  },
};

function createBaseGetGravityTasksRequest(): GetGravityTasksRequest {
  return { gravityTaskId: undefined, includeCrawlers: undefined };
}

export const GetGravityTasksRequest: MessageFns<GetGravityTasksRequest> = {
  encode(
    message: GetGravityTasksRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.gravityTaskId !== undefined) {
      writer.uint32(10).string(message.gravityTaskId);
    }
    if (message.includeCrawlers !== undefined) {
      writer.uint32(16).bool(message.includeCrawlers);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): GetGravityTasksRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetGravityTasksRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.gravityTaskId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.includeCrawlers = reader.bool();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetGravityTasksRequest {
    return {
      gravityTaskId: isSet(object.gravityTaskId)
        ? globalThis.String(object.gravityTaskId)
        : undefined,
      includeCrawlers: isSet(object.includeCrawlers)
        ? globalThis.Boolean(object.includeCrawlers)
        : undefined,
    };
  },

  toJSON(message: GetGravityTasksRequest): unknown {
    const obj: any = {};
    if (message.gravityTaskId !== undefined) {
      obj.gravityTaskId = message.gravityTaskId;
    }
    if (message.includeCrawlers !== undefined) {
      obj.includeCrawlers = message.includeCrawlers;
    }
    return obj;
  },

  create(base?: DeepPartial<GetGravityTasksRequest>): GetGravityTasksRequest {
    return GetGravityTasksRequest.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<GetGravityTasksRequest>,
  ): GetGravityTasksRequest {
    const message = createBaseGetGravityTasksRequest();
    message.gravityTaskId = object.gravityTaskId ?? undefined;
    message.includeCrawlers = object.includeCrawlers ?? undefined;
    return message;
  },
};

function createBaseGetGravityTasksResponse(): GetGravityTasksResponse {
  return { gravityTaskStates: [] };
}

export const GetGravityTasksResponse: MessageFns<GetGravityTasksResponse> = {
  encode(
    message: GetGravityTasksResponse,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    for (const v of message.gravityTaskStates) {
      GravityTaskState.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): GetGravityTasksResponse {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetGravityTasksResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.gravityTaskStates.push(
            GravityTaskState.decode(reader, reader.uint32()),
          );
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetGravityTasksResponse {
    return {
      gravityTaskStates: globalThis.Array.isArray(object?.gravityTaskStates)
        ? object.gravityTaskStates.map((e: any) => GravityTaskState.fromJSON(e))
        : [],
    };
  },

  toJSON(message: GetGravityTasksResponse): unknown {
    const obj: any = {};
    if (message.gravityTaskStates?.length) {
      obj.gravityTaskStates = message.gravityTaskStates.map(e =>
        GravityTaskState.toJSON(e),
      );
    }
    return obj;
  },

  create(base?: DeepPartial<GetGravityTasksResponse>): GetGravityTasksResponse {
    return GetGravityTasksResponse.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<GetGravityTasksResponse>,
  ): GetGravityTasksResponse {
    const message = createBaseGetGravityTasksResponse();
    message.gravityTaskStates =
      object.gravityTaskStates?.map(e => GravityTaskState.fromPartial(e)) || [];
    return message;
  },
};

function createBaseGravityTask(): GravityTask {
  return {
    topic: undefined,
    platform: "",
    keyword: undefined,
    postStartDatetime: undefined,
    postEndDatetime: undefined,
  };
}

export const GravityTask: MessageFns<GravityTask> = {
  encode(
    message: GravityTask,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.topic !== undefined) {
      writer.uint32(10).string(message.topic);
    }
    if (message.platform !== "") {
      writer.uint32(18).string(message.platform);
    }
    if (message.keyword !== undefined) {
      writer.uint32(26).string(message.keyword);
    }
    if (message.postStartDatetime !== undefined) {
      Timestamp.encode(
        toTimestamp(message.postStartDatetime),
        writer.uint32(34).fork(),
      ).join();
    }
    if (message.postEndDatetime !== undefined) {
      Timestamp.encode(
        toTimestamp(message.postEndDatetime),
        writer.uint32(42).fork(),
      ).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GravityTask {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGravityTask();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.topic = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.platform = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.keyword = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.postStartDatetime = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.postEndDatetime = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GravityTask {
    return {
      topic: isSet(object.topic) ? globalThis.String(object.topic) : undefined,
      platform: isSet(object.platform)
        ? globalThis.String(object.platform)
        : "",
      keyword: isSet(object.keyword)
        ? globalThis.String(object.keyword)
        : undefined,
      postStartDatetime: isSet(object.postStartDatetime)
        ? fromJsonTimestamp(object.postStartDatetime)
        : undefined,
      postEndDatetime: isSet(object.postEndDatetime)
        ? fromJsonTimestamp(object.postEndDatetime)
        : undefined,
    };
  },

  toJSON(message: GravityTask): unknown {
    const obj: any = {};
    if (message.topic !== undefined) {
      obj.topic = message.topic;
    }
    if (message.platform !== "") {
      obj.platform = message.platform;
    }
    if (message.keyword !== undefined) {
      obj.keyword = message.keyword;
    }
    if (message.postStartDatetime !== undefined) {
      obj.postStartDatetime = message.postStartDatetime.toISOString();
    }
    if (message.postEndDatetime !== undefined) {
      obj.postEndDatetime = message.postEndDatetime.toISOString();
    }
    return obj;
  },

  create(base?: DeepPartial<GravityTask>): GravityTask {
    return GravityTask.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GravityTask>): GravityTask {
    const message = createBaseGravityTask();
    message.topic = object.topic ?? undefined;
    message.platform = object.platform ?? "";
    message.keyword = object.keyword ?? undefined;
    message.postStartDatetime = object.postStartDatetime ?? undefined;
    message.postEndDatetime = object.postEndDatetime ?? undefined;
    return message;
  },
};

function createBaseNotificationRequest(): NotificationRequest {
  return { type: "", address: "", redirectUrl: undefined };
}

export const NotificationRequest: MessageFns<NotificationRequest> = {
  encode(
    message: NotificationRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.type !== "") {
      writer.uint32(10).string(message.type);
    }
    if (message.address !== "") {
      writer.uint32(18).string(message.address);
    }
    if (message.redirectUrl !== undefined) {
      writer.uint32(26).string(message.redirectUrl);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): NotificationRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNotificationRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.type = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.address = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.redirectUrl = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NotificationRequest {
    return {
      type: isSet(object.type) ? globalThis.String(object.type) : "",
      address: isSet(object.address) ? globalThis.String(object.address) : "",
      redirectUrl: isSet(object.redirectUrl)
        ? globalThis.String(object.redirectUrl)
        : undefined,
    };
  },

  toJSON(message: NotificationRequest): unknown {
    const obj: any = {};
    if (message.type !== "") {
      obj.type = message.type;
    }
    if (message.address !== "") {
      obj.address = message.address;
    }
    if (message.redirectUrl !== undefined) {
      obj.redirectUrl = message.redirectUrl;
    }
    return obj;
  },

  create(base?: DeepPartial<NotificationRequest>): NotificationRequest {
    return NotificationRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<NotificationRequest>): NotificationRequest {
    const message = createBaseNotificationRequest();
    message.type = object.type ?? "";
    message.address = object.address ?? "";
    message.redirectUrl = object.redirectUrl ?? undefined;
    return message;
  },
};

function createBaseGetCrawlerRequest(): GetCrawlerRequest {
  return { crawlerId: "" };
}

export const GetCrawlerRequest: MessageFns<GetCrawlerRequest> = {
  encode(
    message: GetCrawlerRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.crawlerId !== "") {
      writer.uint32(10).string(message.crawlerId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetCrawlerRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetCrawlerRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.crawlerId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetCrawlerRequest {
    return {
      crawlerId: isSet(object.crawlerId)
        ? globalThis.String(object.crawlerId)
        : "",
    };
  },

  toJSON(message: GetCrawlerRequest): unknown {
    const obj: any = {};
    if (message.crawlerId !== "") {
      obj.crawlerId = message.crawlerId;
    }
    return obj;
  },

  create(base?: DeepPartial<GetCrawlerRequest>): GetCrawlerRequest {
    return GetCrawlerRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetCrawlerRequest>): GetCrawlerRequest {
    const message = createBaseGetCrawlerRequest();
    message.crawlerId = object.crawlerId ?? "";
    return message;
  },
};

function createBaseGetCrawlerResponse(): GetCrawlerResponse {
  return { crawler: undefined };
}

export const GetCrawlerResponse: MessageFns<GetCrawlerResponse> = {
  encode(
    message: GetCrawlerResponse,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.crawler !== undefined) {
      Crawler.encode(message.crawler, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): GetCrawlerResponse {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetCrawlerResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.crawler = Crawler.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetCrawlerResponse {
    return {
      crawler: isSet(object.crawler)
        ? Crawler.fromJSON(object.crawler)
        : undefined,
    };
  },

  toJSON(message: GetCrawlerResponse): unknown {
    const obj: any = {};
    if (message.crawler !== undefined) {
      obj.crawler = Crawler.toJSON(message.crawler);
    }
    return obj;
  },

  create(base?: DeepPartial<GetCrawlerResponse>): GetCrawlerResponse {
    return GetCrawlerResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetCrawlerResponse>): GetCrawlerResponse {
    const message = createBaseGetCrawlerResponse();
    message.crawler =
      object.crawler !== undefined && object.crawler !== null
        ? Crawler.fromPartial(object.crawler)
        : undefined;
    return message;
  },
};

function createBaseCreateGravityTaskRequest(): CreateGravityTaskRequest {
  return {
    gravityTasks: [],
    name: "",
    notificationRequests: [],
    gravityTaskId: undefined,
  };
}

export const CreateGravityTaskRequest: MessageFns<CreateGravityTaskRequest> = {
  encode(
    message: CreateGravityTaskRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    for (const v of message.gravityTasks) {
      GravityTask.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.name !== "") {
      writer.uint32(18).string(message.name);
    }
    for (const v of message.notificationRequests) {
      NotificationRequest.encode(v!, writer.uint32(26).fork()).join();
    }
    if (message.gravityTaskId !== undefined) {
      writer.uint32(34).string(message.gravityTaskId);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): CreateGravityTaskRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCreateGravityTaskRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.gravityTasks.push(
            GravityTask.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.name = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.notificationRequests.push(
            NotificationRequest.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.gravityTaskId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CreateGravityTaskRequest {
    return {
      gravityTasks: globalThis.Array.isArray(object?.gravityTasks)
        ? object.gravityTasks.map((e: any) => GravityTask.fromJSON(e))
        : [],
      name: isSet(object.name) ? globalThis.String(object.name) : "",
      notificationRequests: globalThis.Array.isArray(
        object?.notificationRequests,
      )
        ? object.notificationRequests.map((e: any) =>
            NotificationRequest.fromJSON(e),
          )
        : [],
      gravityTaskId: isSet(object.gravityTaskId)
        ? globalThis.String(object.gravityTaskId)
        : undefined,
    };
  },

  toJSON(message: CreateGravityTaskRequest): unknown {
    const obj: any = {};
    if (message.gravityTasks?.length) {
      obj.gravityTasks = message.gravityTasks.map(e => GravityTask.toJSON(e));
    }
    if (message.name !== "") {
      obj.name = message.name;
    }
    if (message.notificationRequests?.length) {
      obj.notificationRequests = message.notificationRequests.map(e =>
        NotificationRequest.toJSON(e),
      );
    }
    if (message.gravityTaskId !== undefined) {
      obj.gravityTaskId = message.gravityTaskId;
    }
    return obj;
  },

  create(
    base?: DeepPartial<CreateGravityTaskRequest>,
  ): CreateGravityTaskRequest {
    return CreateGravityTaskRequest.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<CreateGravityTaskRequest>,
  ): CreateGravityTaskRequest {
    const message = createBaseCreateGravityTaskRequest();
    message.gravityTasks =
      object.gravityTasks?.map(e => GravityTask.fromPartial(e)) || [];
    message.name = object.name ?? "";
    message.notificationRequests =
      object.notificationRequests?.map(e =>
        NotificationRequest.fromPartial(e),
      ) || [];
    message.gravityTaskId = object.gravityTaskId ?? undefined;
    return message;
  },
};

function createBaseCreateGravityTaskResponse(): CreateGravityTaskResponse {
  return { gravityTaskId: "" };
}

export const CreateGravityTaskResponse: MessageFns<CreateGravityTaskResponse> =
  {
    encode(
      message: CreateGravityTaskResponse,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.gravityTaskId !== "") {
        writer.uint32(10).string(message.gravityTaskId);
      }
      return writer;
    },

    decode(
      input: BinaryReader | Uint8Array,
      length?: number,
    ): CreateGravityTaskResponse {
      const reader =
        input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseCreateGravityTaskResponse();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1: {
            if (tag !== 10) {
              break;
            }

            message.gravityTaskId = reader.string();
            continue;
          }
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): CreateGravityTaskResponse {
      return {
        gravityTaskId: isSet(object.gravityTaskId)
          ? globalThis.String(object.gravityTaskId)
          : "",
      };
    },

    toJSON(message: CreateGravityTaskResponse): unknown {
      const obj: any = {};
      if (message.gravityTaskId !== "") {
        obj.gravityTaskId = message.gravityTaskId;
      }
      return obj;
    },

    create(
      base?: DeepPartial<CreateGravityTaskResponse>,
    ): CreateGravityTaskResponse {
      return CreateGravityTaskResponse.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<CreateGravityTaskResponse>,
    ): CreateGravityTaskResponse {
      const message = createBaseCreateGravityTaskResponse();
      message.gravityTaskId = object.gravityTaskId ?? "";
      return message;
    },
  };

function createBaseBuildDatasetRequest(): BuildDatasetRequest {
  return { crawlerId: "", notificationRequests: [], maxRows: 0 };
}

export const BuildDatasetRequest: MessageFns<BuildDatasetRequest> = {
  encode(
    message: BuildDatasetRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.crawlerId !== "") {
      writer.uint32(10).string(message.crawlerId);
    }
    for (const v of message.notificationRequests) {
      NotificationRequest.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.maxRows !== 0) {
      writer.uint32(24).int64(message.maxRows);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): BuildDatasetRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBuildDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.crawlerId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.notificationRequests.push(
            NotificationRequest.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.maxRows = longToNumber(reader.int64());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BuildDatasetRequest {
    return {
      crawlerId: isSet(object.crawlerId)
        ? globalThis.String(object.crawlerId)
        : "",
      notificationRequests: globalThis.Array.isArray(
        object?.notificationRequests,
      )
        ? object.notificationRequests.map((e: any) =>
            NotificationRequest.fromJSON(e),
          )
        : [],
      maxRows: isSet(object.maxRows) ? globalThis.Number(object.maxRows) : 0,
    };
  },

  toJSON(message: BuildDatasetRequest): unknown {
    const obj: any = {};
    if (message.crawlerId !== "") {
      obj.crawlerId = message.crawlerId;
    }
    if (message.notificationRequests?.length) {
      obj.notificationRequests = message.notificationRequests.map(e =>
        NotificationRequest.toJSON(e),
      );
    }
    if (message.maxRows !== 0) {
      obj.maxRows = Math.round(message.maxRows);
    }
    return obj;
  },

  create(base?: DeepPartial<BuildDatasetRequest>): BuildDatasetRequest {
    return BuildDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BuildDatasetRequest>): BuildDatasetRequest {
    const message = createBaseBuildDatasetRequest();
    message.crawlerId = object.crawlerId ?? "";
    message.notificationRequests =
      object.notificationRequests?.map(e =>
        NotificationRequest.fromPartial(e),
      ) || [];
    message.maxRows = object.maxRows ?? 0;
    return message;
  },
};

function createBaseBuildDatasetResponse(): BuildDatasetResponse {
  return { datasetId: "", dataset: undefined };
}

export const BuildDatasetResponse: MessageFns<BuildDatasetResponse> = {
  encode(
    message: BuildDatasetResponse,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    if (message.dataset !== undefined) {
      Dataset.encode(message.dataset, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): BuildDatasetResponse {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBuildDatasetResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.dataset = Dataset.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BuildDatasetResponse {
    return {
      datasetId: isSet(object.datasetId)
        ? globalThis.String(object.datasetId)
        : "",
      dataset: isSet(object.dataset)
        ? Dataset.fromJSON(object.dataset)
        : undefined,
    };
  },

  toJSON(message: BuildDatasetResponse): unknown {
    const obj: any = {};
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.dataset !== undefined) {
      obj.dataset = Dataset.toJSON(message.dataset);
    }
    return obj;
  },

  create(base?: DeepPartial<BuildDatasetResponse>): BuildDatasetResponse {
    return BuildDatasetResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<BuildDatasetResponse>): BuildDatasetResponse {
    const message = createBaseBuildDatasetResponse();
    message.datasetId = object.datasetId ?? "";
    message.dataset =
      object.dataset !== undefined && object.dataset !== null
        ? Dataset.fromPartial(object.dataset)
        : undefined;
    return message;
  },
};

function createBaseBuildAllDatasetsRequest(): BuildAllDatasetsRequest {
  return { gravityTaskId: "", buildCrawlersConfig: [] };
}

export const BuildAllDatasetsRequest: MessageFns<BuildAllDatasetsRequest> = {
  encode(
    message: BuildAllDatasetsRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.gravityTaskId !== "") {
      writer.uint32(10).string(message.gravityTaskId);
    }
    for (const v of message.buildCrawlersConfig) {
      BuildDatasetRequest.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): BuildAllDatasetsRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBuildAllDatasetsRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.gravityTaskId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.buildCrawlersConfig.push(
            BuildDatasetRequest.decode(reader, reader.uint32()),
          );
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BuildAllDatasetsRequest {
    return {
      gravityTaskId: isSet(object.gravityTaskId)
        ? globalThis.String(object.gravityTaskId)
        : "",
      buildCrawlersConfig: globalThis.Array.isArray(object?.buildCrawlersConfig)
        ? object.buildCrawlersConfig.map((e: any) =>
            BuildDatasetRequest.fromJSON(e),
          )
        : [],
    };
  },

  toJSON(message: BuildAllDatasetsRequest): unknown {
    const obj: any = {};
    if (message.gravityTaskId !== "") {
      obj.gravityTaskId = message.gravityTaskId;
    }
    if (message.buildCrawlersConfig?.length) {
      obj.buildCrawlersConfig = message.buildCrawlersConfig.map(e =>
        BuildDatasetRequest.toJSON(e),
      );
    }
    return obj;
  },

  create(base?: DeepPartial<BuildAllDatasetsRequest>): BuildAllDatasetsRequest {
    return BuildAllDatasetsRequest.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<BuildAllDatasetsRequest>,
  ): BuildAllDatasetsRequest {
    const message = createBaseBuildAllDatasetsRequest();
    message.gravityTaskId = object.gravityTaskId ?? "";
    message.buildCrawlersConfig =
      object.buildCrawlersConfig?.map(e =>
        BuildDatasetRequest.fromPartial(e),
      ) || [];
    return message;
  },
};

function createBaseBuildAllDatasetsResponse(): BuildAllDatasetsResponse {
  return { gravityTaskId: "", datasets: [] };
}

export const BuildAllDatasetsResponse: MessageFns<BuildAllDatasetsResponse> = {
  encode(
    message: BuildAllDatasetsResponse,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.gravityTaskId !== "") {
      writer.uint32(10).string(message.gravityTaskId);
    }
    for (const v of message.datasets) {
      Dataset.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): BuildAllDatasetsResponse {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseBuildAllDatasetsResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.gravityTaskId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.datasets.push(Dataset.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): BuildAllDatasetsResponse {
    return {
      gravityTaskId: isSet(object.gravityTaskId)
        ? globalThis.String(object.gravityTaskId)
        : "",
      datasets: globalThis.Array.isArray(object?.datasets)
        ? object.datasets.map((e: any) => Dataset.fromJSON(e))
        : [],
    };
  },

  toJSON(message: BuildAllDatasetsResponse): unknown {
    const obj: any = {};
    if (message.gravityTaskId !== "") {
      obj.gravityTaskId = message.gravityTaskId;
    }
    if (message.datasets?.length) {
      obj.datasets = message.datasets.map(e => Dataset.toJSON(e));
    }
    return obj;
  },

  create(
    base?: DeepPartial<BuildAllDatasetsResponse>,
  ): BuildAllDatasetsResponse {
    return BuildAllDatasetsResponse.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<BuildAllDatasetsResponse>,
  ): BuildAllDatasetsResponse {
    const message = createBaseBuildAllDatasetsResponse();
    message.gravityTaskId = object.gravityTaskId ?? "";
    message.datasets = object.datasets?.map(e => Dataset.fromPartial(e)) || [];
    return message;
  },
};

function createBaseNebula(): Nebula {
  return { error: "", fileSizeBytes: 0, url: "" };
}

export const Nebula: MessageFns<Nebula> = {
  encode(
    message: Nebula,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.error !== "") {
      writer.uint32(10).string(message.error);
    }
    if (message.fileSizeBytes !== 0) {
      writer.uint32(16).int64(message.fileSizeBytes);
    }
    if (message.url !== "") {
      writer.uint32(26).string(message.url);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Nebula {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNebula();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.error = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.fileSizeBytes = longToNumber(reader.int64());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.url = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Nebula {
    return {
      error: isSet(object.error) ? globalThis.String(object.error) : "",
      fileSizeBytes: isSet(object.fileSizeBytes)
        ? globalThis.Number(object.fileSizeBytes)
        : 0,
      url: isSet(object.url) ? globalThis.String(object.url) : "",
    };
  },

  toJSON(message: Nebula): unknown {
    const obj: any = {};
    if (message.error !== "") {
      obj.error = message.error;
    }
    if (message.fileSizeBytes !== 0) {
      obj.fileSizeBytes = Math.round(message.fileSizeBytes);
    }
    if (message.url !== "") {
      obj.url = message.url;
    }
    return obj;
  },

  create(base?: DeepPartial<Nebula>): Nebula {
    return Nebula.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Nebula>): Nebula {
    const message = createBaseNebula();
    message.error = object.error ?? "";
    message.fileSizeBytes = object.fileSizeBytes ?? 0;
    message.url = object.url ?? "";
    return message;
  },
};

function createBaseDataset(): Dataset {
  return {
    crawlerWorkflowId: "",
    createDate: undefined,
    expireDate: undefined,
    files: [],
    status: "",
    statusMessage: "",
    steps: [],
    totalSteps: 0,
    nebula: undefined,
  };
}

export const Dataset: MessageFns<Dataset> = {
  encode(
    message: Dataset,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.crawlerWorkflowId !== "") {
      writer.uint32(10).string(message.crawlerWorkflowId);
    }
    if (message.createDate !== undefined) {
      Timestamp.encode(
        toTimestamp(message.createDate),
        writer.uint32(18).fork(),
      ).join();
    }
    if (message.expireDate !== undefined) {
      Timestamp.encode(
        toTimestamp(message.expireDate),
        writer.uint32(26).fork(),
      ).join();
    }
    for (const v of message.files) {
      DatasetFile.encode(v!, writer.uint32(34).fork()).join();
    }
    if (message.status !== "") {
      writer.uint32(42).string(message.status);
    }
    if (message.statusMessage !== "") {
      writer.uint32(50).string(message.statusMessage);
    }
    for (const v of message.steps) {
      DatasetStep.encode(v!, writer.uint32(58).fork()).join();
    }
    if (message.totalSteps !== 0) {
      writer.uint32(64).int64(message.totalSteps);
    }
    if (message.nebula !== undefined) {
      Nebula.encode(message.nebula, writer.uint32(74).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Dataset {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDataset();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.crawlerWorkflowId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.createDate = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.expireDate = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.files.push(DatasetFile.decode(reader, reader.uint32()));
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.status = reader.string();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.statusMessage = reader.string();
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.steps.push(DatasetStep.decode(reader, reader.uint32()));
          continue;
        }
        case 8: {
          if (tag !== 64) {
            break;
          }

          message.totalSteps = longToNumber(reader.int64());
          continue;
        }
        case 9: {
          if (tag !== 74) {
            break;
          }

          message.nebula = Nebula.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Dataset {
    return {
      crawlerWorkflowId: isSet(object.crawlerWorkflowId)
        ? globalThis.String(object.crawlerWorkflowId)
        : "",
      createDate: isSet(object.createDate)
        ? fromJsonTimestamp(object.createDate)
        : undefined,
      expireDate: isSet(object.expireDate)
        ? fromJsonTimestamp(object.expireDate)
        : undefined,
      files: globalThis.Array.isArray(object?.files)
        ? object.files.map((e: any) => DatasetFile.fromJSON(e))
        : [],
      status: isSet(object.status) ? globalThis.String(object.status) : "",
      statusMessage: isSet(object.statusMessage)
        ? globalThis.String(object.statusMessage)
        : "",
      steps: globalThis.Array.isArray(object?.steps)
        ? object.steps.map((e: any) => DatasetStep.fromJSON(e))
        : [],
      totalSteps: isSet(object.totalSteps)
        ? globalThis.Number(object.totalSteps)
        : 0,
      nebula: isSet(object.nebula) ? Nebula.fromJSON(object.nebula) : undefined,
    };
  },

  toJSON(message: Dataset): unknown {
    const obj: any = {};
    if (message.crawlerWorkflowId !== "") {
      obj.crawlerWorkflowId = message.crawlerWorkflowId;
    }
    if (message.createDate !== undefined) {
      obj.createDate = message.createDate.toISOString();
    }
    if (message.expireDate !== undefined) {
      obj.expireDate = message.expireDate.toISOString();
    }
    if (message.files?.length) {
      obj.files = message.files.map(e => DatasetFile.toJSON(e));
    }
    if (message.status !== "") {
      obj.status = message.status;
    }
    if (message.statusMessage !== "") {
      obj.statusMessage = message.statusMessage;
    }
    if (message.steps?.length) {
      obj.steps = message.steps.map(e => DatasetStep.toJSON(e));
    }
    if (message.totalSteps !== 0) {
      obj.totalSteps = Math.round(message.totalSteps);
    }
    if (message.nebula !== undefined) {
      obj.nebula = Nebula.toJSON(message.nebula);
    }
    return obj;
  },

  create(base?: DeepPartial<Dataset>): Dataset {
    return Dataset.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Dataset>): Dataset {
    const message = createBaseDataset();
    message.crawlerWorkflowId = object.crawlerWorkflowId ?? "";
    message.createDate = object.createDate ?? undefined;
    message.expireDate = object.expireDate ?? undefined;
    message.files = object.files?.map(e => DatasetFile.fromPartial(e)) || [];
    message.status = object.status ?? "";
    message.statusMessage = object.statusMessage ?? "";
    message.steps = object.steps?.map(e => DatasetStep.fromPartial(e)) || [];
    message.totalSteps = object.totalSteps ?? 0;
    message.nebula =
      object.nebula !== undefined && object.nebula !== null
        ? Nebula.fromPartial(object.nebula)
        : undefined;
    return message;
  },
};

function createBaseUpsertDatasetRequest(): UpsertDatasetRequest {
  return { datasetId: "", dataset: undefined };
}

export const UpsertDatasetRequest: MessageFns<UpsertDatasetRequest> = {
  encode(
    message: UpsertDatasetRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    if (message.dataset !== undefined) {
      Dataset.encode(message.dataset, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): UpsertDatasetRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpsertDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.dataset = Dataset.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpsertDatasetRequest {
    return {
      datasetId: isSet(object.datasetId)
        ? globalThis.String(object.datasetId)
        : "",
      dataset: isSet(object.dataset)
        ? Dataset.fromJSON(object.dataset)
        : undefined,
    };
  },

  toJSON(message: UpsertDatasetRequest): unknown {
    const obj: any = {};
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.dataset !== undefined) {
      obj.dataset = Dataset.toJSON(message.dataset);
    }
    return obj;
  },

  create(base?: DeepPartial<UpsertDatasetRequest>): UpsertDatasetRequest {
    return UpsertDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpsertDatasetRequest>): UpsertDatasetRequest {
    const message = createBaseUpsertDatasetRequest();
    message.datasetId = object.datasetId ?? "";
    message.dataset =
      object.dataset !== undefined && object.dataset !== null
        ? Dataset.fromPartial(object.dataset)
        : undefined;
    return message;
  },
};

function createBaseUpsertNebulaRequest(): UpsertNebulaRequest {
  return { datasetId: "", nebulaId: "", nebula: undefined };
}

export const UpsertNebulaRequest: MessageFns<UpsertNebulaRequest> = {
  encode(
    message: UpsertNebulaRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    if (message.nebulaId !== "") {
      writer.uint32(18).string(message.nebulaId);
    }
    if (message.nebula !== undefined) {
      Nebula.encode(message.nebula, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): UpsertNebulaRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseUpsertNebulaRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.nebulaId = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.nebula = Nebula.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): UpsertNebulaRequest {
    return {
      datasetId: isSet(object.datasetId)
        ? globalThis.String(object.datasetId)
        : "",
      nebulaId: isSet(object.nebulaId)
        ? globalThis.String(object.nebulaId)
        : "",
      nebula: isSet(object.nebula) ? Nebula.fromJSON(object.nebula) : undefined,
    };
  },

  toJSON(message: UpsertNebulaRequest): unknown {
    const obj: any = {};
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.nebulaId !== "") {
      obj.nebulaId = message.nebulaId;
    }
    if (message.nebula !== undefined) {
      obj.nebula = Nebula.toJSON(message.nebula);
    }
    return obj;
  },

  create(base?: DeepPartial<UpsertNebulaRequest>): UpsertNebulaRequest {
    return UpsertNebulaRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<UpsertNebulaRequest>): UpsertNebulaRequest {
    const message = createBaseUpsertNebulaRequest();
    message.datasetId = object.datasetId ?? "";
    message.nebulaId = object.nebulaId ?? "";
    message.nebula =
      object.nebula !== undefined && object.nebula !== null
        ? Nebula.fromPartial(object.nebula)
        : undefined;
    return message;
  },
};

function createBaseInsertDatasetFileRequest(): InsertDatasetFileRequest {
  return { datasetId: "", files: [] };
}

export const InsertDatasetFileRequest: MessageFns<InsertDatasetFileRequest> = {
  encode(
    message: InsertDatasetFileRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    for (const v of message.files) {
      DatasetFile.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): InsertDatasetFileRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseInsertDatasetFileRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.files.push(DatasetFile.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): InsertDatasetFileRequest {
    return {
      datasetId: isSet(object.datasetId)
        ? globalThis.String(object.datasetId)
        : "",
      files: globalThis.Array.isArray(object?.files)
        ? object.files.map((e: any) => DatasetFile.fromJSON(e))
        : [],
    };
  },

  toJSON(message: InsertDatasetFileRequest): unknown {
    const obj: any = {};
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.files?.length) {
      obj.files = message.files.map(e => DatasetFile.toJSON(e));
    }
    return obj;
  },

  create(
    base?: DeepPartial<InsertDatasetFileRequest>,
  ): InsertDatasetFileRequest {
    return InsertDatasetFileRequest.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<InsertDatasetFileRequest>,
  ): InsertDatasetFileRequest {
    const message = createBaseInsertDatasetFileRequest();
    message.datasetId = object.datasetId ?? "";
    message.files = object.files?.map(e => DatasetFile.fromPartial(e)) || [];
    return message;
  },
};

function createBaseDatasetFile(): DatasetFile {
  return {
    fileName: "",
    fileSizeBytes: 0,
    lastModified: undefined,
    numRows: 0,
    s3Key: "",
    url: "",
  };
}

export const DatasetFile: MessageFns<DatasetFile> = {
  encode(
    message: DatasetFile,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.fileName !== "") {
      writer.uint32(10).string(message.fileName);
    }
    if (message.fileSizeBytes !== 0) {
      writer.uint32(16).uint64(message.fileSizeBytes);
    }
    if (message.lastModified !== undefined) {
      Timestamp.encode(
        toTimestamp(message.lastModified),
        writer.uint32(26).fork(),
      ).join();
    }
    if (message.numRows !== 0) {
      writer.uint32(32).uint64(message.numRows);
    }
    if (message.s3Key !== "") {
      writer.uint32(42).string(message.s3Key);
    }
    if (message.url !== "") {
      writer.uint32(50).string(message.url);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetFile {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetFile();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.fileName = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.fileSizeBytes = longToNumber(reader.uint64());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.lastModified = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.numRows = longToNumber(reader.uint64());
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          message.s3Key = reader.string();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.url = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatasetFile {
    return {
      fileName: isSet(object.fileName)
        ? globalThis.String(object.fileName)
        : "",
      fileSizeBytes: isSet(object.fileSizeBytes)
        ? globalThis.Number(object.fileSizeBytes)
        : 0,
      lastModified: isSet(object.lastModified)
        ? fromJsonTimestamp(object.lastModified)
        : undefined,
      numRows: isSet(object.numRows) ? globalThis.Number(object.numRows) : 0,
      s3Key: isSet(object.s3Key) ? globalThis.String(object.s3Key) : "",
      url: isSet(object.url) ? globalThis.String(object.url) : "",
    };
  },

  toJSON(message: DatasetFile): unknown {
    const obj: any = {};
    if (message.fileName !== "") {
      obj.fileName = message.fileName;
    }
    if (message.fileSizeBytes !== 0) {
      obj.fileSizeBytes = Math.round(message.fileSizeBytes);
    }
    if (message.lastModified !== undefined) {
      obj.lastModified = message.lastModified.toISOString();
    }
    if (message.numRows !== 0) {
      obj.numRows = Math.round(message.numRows);
    }
    if (message.s3Key !== "") {
      obj.s3Key = message.s3Key;
    }
    if (message.url !== "") {
      obj.url = message.url;
    }
    return obj;
  },

  create(base?: DeepPartial<DatasetFile>): DatasetFile {
    return DatasetFile.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetFile>): DatasetFile {
    const message = createBaseDatasetFile();
    message.fileName = object.fileName ?? "";
    message.fileSizeBytes = object.fileSizeBytes ?? 0;
    message.lastModified = object.lastModified ?? undefined;
    message.numRows = object.numRows ?? 0;
    message.s3Key = object.s3Key ?? "";
    message.url = object.url ?? "";
    return message;
  },
};

function createBaseDatasetStep(): DatasetStep {
  return { progress: 0, step: 0, stepName: "" };
}

export const DatasetStep: MessageFns<DatasetStep> = {
  encode(
    message: DatasetStep,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.progress !== 0) {
      writer.uint32(9).double(message.progress);
    }
    if (message.step !== 0) {
      writer.uint32(16).int64(message.step);
    }
    if (message.stepName !== "") {
      writer.uint32(26).string(message.stepName);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetStep {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetStep();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 9) {
            break;
          }

          message.progress = reader.double();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.step = longToNumber(reader.int64());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.stepName = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatasetStep {
    return {
      progress: isSet(object.progress) ? globalThis.Number(object.progress) : 0,
      step: isSet(object.step) ? globalThis.Number(object.step) : 0,
      stepName: isSet(object.stepName)
        ? globalThis.String(object.stepName)
        : "",
    };
  },

  toJSON(message: DatasetStep): unknown {
    const obj: any = {};
    if (message.progress !== 0) {
      obj.progress = message.progress;
    }
    if (message.step !== 0) {
      obj.step = Math.round(message.step);
    }
    if (message.stepName !== "") {
      obj.stepName = message.stepName;
    }
    return obj;
  },

  create(base?: DeepPartial<DatasetStep>): DatasetStep {
    return DatasetStep.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetStep>): DatasetStep {
    const message = createBaseDatasetStep();
    message.progress = object.progress ?? 0;
    message.step = object.step ?? 0;
    message.stepName = object.stepName ?? "";
    return message;
  },
};

function createBaseGetDatasetRequest(): GetDatasetRequest {
  return { datasetId: "" };
}

export const GetDatasetRequest: MessageFns<GetDatasetRequest> = {
  encode(
    message: GetDatasetRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): GetDatasetRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetDatasetRequest {
    return {
      datasetId: isSet(object.datasetId)
        ? globalThis.String(object.datasetId)
        : "",
    };
  },

  toJSON(message: GetDatasetRequest): unknown {
    const obj: any = {};
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    return obj;
  },

  create(base?: DeepPartial<GetDatasetRequest>): GetDatasetRequest {
    return GetDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetDatasetRequest>): GetDatasetRequest {
    const message = createBaseGetDatasetRequest();
    message.datasetId = object.datasetId ?? "";
    return message;
  },
};

function createBaseGetDatasetResponse(): GetDatasetResponse {
  return { dataset: undefined };
}

export const GetDatasetResponse: MessageFns<GetDatasetResponse> = {
  encode(
    message: GetDatasetResponse,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.dataset !== undefined) {
      Dataset.encode(message.dataset, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): GetDatasetResponse {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseGetDatasetResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.dataset = Dataset.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): GetDatasetResponse {
    return {
      dataset: isSet(object.dataset)
        ? Dataset.fromJSON(object.dataset)
        : undefined,
    };
  },

  toJSON(message: GetDatasetResponse): unknown {
    const obj: any = {};
    if (message.dataset !== undefined) {
      obj.dataset = Dataset.toJSON(message.dataset);
    }
    return obj;
  },

  create(base?: DeepPartial<GetDatasetResponse>): GetDatasetResponse {
    return GetDatasetResponse.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<GetDatasetResponse>): GetDatasetResponse {
    const message = createBaseGetDatasetResponse();
    message.dataset =
      object.dataset !== undefined && object.dataset !== null
        ? Dataset.fromPartial(object.dataset)
        : undefined;
    return message;
  },
};

function createBaseCancelGravityTaskRequest(): CancelGravityTaskRequest {
  return { gravityTaskId: "" };
}

export const CancelGravityTaskRequest: MessageFns<CancelGravityTaskRequest> = {
  encode(
    message: CancelGravityTaskRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.gravityTaskId !== "") {
      writer.uint32(10).string(message.gravityTaskId);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): CancelGravityTaskRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCancelGravityTaskRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.gravityTaskId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CancelGravityTaskRequest {
    return {
      gravityTaskId: isSet(object.gravityTaskId)
        ? globalThis.String(object.gravityTaskId)
        : "",
    };
  },

  toJSON(message: CancelGravityTaskRequest): unknown {
    const obj: any = {};
    if (message.gravityTaskId !== "") {
      obj.gravityTaskId = message.gravityTaskId;
    }
    return obj;
  },

  create(
    base?: DeepPartial<CancelGravityTaskRequest>,
  ): CancelGravityTaskRequest {
    return CancelGravityTaskRequest.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<CancelGravityTaskRequest>,
  ): CancelGravityTaskRequest {
    const message = createBaseCancelGravityTaskRequest();
    message.gravityTaskId = object.gravityTaskId ?? "";
    return message;
  },
};

function createBaseCancelGravityTaskResponse(): CancelGravityTaskResponse {
  return { message: "" };
}

export const CancelGravityTaskResponse: MessageFns<CancelGravityTaskResponse> =
  {
    encode(
      message: CancelGravityTaskResponse,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.message !== "") {
        writer.uint32(10).string(message.message);
      }
      return writer;
    },

    decode(
      input: BinaryReader | Uint8Array,
      length?: number,
    ): CancelGravityTaskResponse {
      const reader =
        input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseCancelGravityTaskResponse();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1: {
            if (tag !== 10) {
              break;
            }

            message.message = reader.string();
            continue;
          }
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): CancelGravityTaskResponse {
      return {
        message: isSet(object.message) ? globalThis.String(object.message) : "",
      };
    },

    toJSON(message: CancelGravityTaskResponse): unknown {
      const obj: any = {};
      if (message.message !== "") {
        obj.message = message.message;
      }
      return obj;
    },

    create(
      base?: DeepPartial<CancelGravityTaskResponse>,
    ): CancelGravityTaskResponse {
      return CancelGravityTaskResponse.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<CancelGravityTaskResponse>,
    ): CancelGravityTaskResponse {
      const message = createBaseCancelGravityTaskResponse();
      message.message = object.message ?? "";
      return message;
    },
  };

function createBaseCancelDatasetRequest(): CancelDatasetRequest {
  return { datasetId: "" };
}

export const CancelDatasetRequest: MessageFns<CancelDatasetRequest> = {
  encode(
    message: CancelDatasetRequest,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): CancelDatasetRequest {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCancelDatasetRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CancelDatasetRequest {
    return {
      datasetId: isSet(object.datasetId)
        ? globalThis.String(object.datasetId)
        : "",
    };
  },

  toJSON(message: CancelDatasetRequest): unknown {
    const obj: any = {};
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    return obj;
  },

  create(base?: DeepPartial<CancelDatasetRequest>): CancelDatasetRequest {
    return CancelDatasetRequest.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CancelDatasetRequest>): CancelDatasetRequest {
    const message = createBaseCancelDatasetRequest();
    message.datasetId = object.datasetId ?? "";
    return message;
  },
};

function createBaseCancelDatasetResponse(): CancelDatasetResponse {
  return { message: "" };
}

export const CancelDatasetResponse: MessageFns<CancelDatasetResponse> = {
  encode(
    message: CancelDatasetResponse,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.message !== "") {
      writer.uint32(10).string(message.message);
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): CancelDatasetResponse {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCancelDatasetResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.message = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CancelDatasetResponse {
    return {
      message: isSet(object.message) ? globalThis.String(object.message) : "",
    };
  },

  toJSON(message: CancelDatasetResponse): unknown {
    const obj: any = {};
    if (message.message !== "") {
      obj.message = message.message;
    }
    return obj;
  },

  create(base?: DeepPartial<CancelDatasetResponse>): CancelDatasetResponse {
    return CancelDatasetResponse.fromPartial(base ?? {});
  },
  fromPartial(
    object: DeepPartial<CancelDatasetResponse>,
  ): CancelDatasetResponse {
    const message = createBaseCancelDatasetResponse();
    message.message = object.message ?? "";
    return message;
  },
};

function createBaseDatasetBillingCorrectionRequest(): DatasetBillingCorrectionRequest {
  return { requestedRowCount: 0, actualRowCount: 0 };
}

export const DatasetBillingCorrectionRequest: MessageFns<DatasetBillingCorrectionRequest> =
  {
    encode(
      message: DatasetBillingCorrectionRequest,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.requestedRowCount !== 0) {
        writer.uint32(8).int64(message.requestedRowCount);
      }
      if (message.actualRowCount !== 0) {
        writer.uint32(16).int64(message.actualRowCount);
      }
      return writer;
    },

    decode(
      input: BinaryReader | Uint8Array,
      length?: number,
    ): DatasetBillingCorrectionRequest {
      const reader =
        input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseDatasetBillingCorrectionRequest();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1: {
            if (tag !== 8) {
              break;
            }

            message.requestedRowCount = longToNumber(reader.int64());
            continue;
          }
          case 2: {
            if (tag !== 16) {
              break;
            }

            message.actualRowCount = longToNumber(reader.int64());
            continue;
          }
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): DatasetBillingCorrectionRequest {
      return {
        requestedRowCount: isSet(object.requestedRowCount)
          ? globalThis.Number(object.requestedRowCount)
          : 0,
        actualRowCount: isSet(object.actualRowCount)
          ? globalThis.Number(object.actualRowCount)
          : 0,
      };
    },

    toJSON(message: DatasetBillingCorrectionRequest): unknown {
      const obj: any = {};
      if (message.requestedRowCount !== 0) {
        obj.requestedRowCount = Math.round(message.requestedRowCount);
      }
      if (message.actualRowCount !== 0) {
        obj.actualRowCount = Math.round(message.actualRowCount);
      }
      return obj;
    },

    create(
      base?: DeepPartial<DatasetBillingCorrectionRequest>,
    ): DatasetBillingCorrectionRequest {
      return DatasetBillingCorrectionRequest.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<DatasetBillingCorrectionRequest>,
    ): DatasetBillingCorrectionRequest {
      const message = createBaseDatasetBillingCorrectionRequest();
      message.requestedRowCount = object.requestedRowCount ?? 0;
      message.actualRowCount = object.actualRowCount ?? 0;
      return message;
    },
  };

function createBaseDatasetBillingCorrectionResponse(): DatasetBillingCorrectionResponse {
  return { refundAmount: 0 };
}

export const DatasetBillingCorrectionResponse: MessageFns<DatasetBillingCorrectionResponse> =
  {
    encode(
      message: DatasetBillingCorrectionResponse,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.refundAmount !== 0) {
        writer.uint32(9).double(message.refundAmount);
      }
      return writer;
    },

    decode(
      input: BinaryReader | Uint8Array,
      length?: number,
    ): DatasetBillingCorrectionResponse {
      const reader =
        input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseDatasetBillingCorrectionResponse();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1: {
            if (tag !== 9) {
              break;
            }

            message.refundAmount = reader.double();
            continue;
          }
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): DatasetBillingCorrectionResponse {
      return {
        refundAmount: isSet(object.refundAmount)
          ? globalThis.Number(object.refundAmount)
          : 0,
      };
    },

    toJSON(message: DatasetBillingCorrectionResponse): unknown {
      const obj: any = {};
      if (message.refundAmount !== 0) {
        obj.refundAmount = message.refundAmount;
      }
      return obj;
    },

    create(
      base?: DeepPartial<DatasetBillingCorrectionResponse>,
    ): DatasetBillingCorrectionResponse {
      return DatasetBillingCorrectionResponse.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<DatasetBillingCorrectionResponse>,
    ): DatasetBillingCorrectionResponse {
      const message = createBaseDatasetBillingCorrectionResponse();
      message.refundAmount = object.refundAmount ?? 0;
      return message;
    },
  };

function createBaseGetMarketplaceDatasetsResponse(): GetMarketplaceDatasetsResponse {
  return { datasets: [] };
}

export const GetMarketplaceDatasetsResponse: MessageFns<GetMarketplaceDatasetsResponse> =
  {
    encode(
      message: GetMarketplaceDatasetsResponse,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      for (const v of message.datasets) {
        DatasetFile.encode(v!, writer.uint32(10).fork()).join();
      }
      return writer;
    },

    decode(
      input: BinaryReader | Uint8Array,
      length?: number,
    ): GetMarketplaceDatasetsResponse {
      const reader =
        input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseGetMarketplaceDatasetsResponse();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1: {
            if (tag !== 10) {
              break;
            }

            message.datasets.push(DatasetFile.decode(reader, reader.uint32()));
            continue;
          }
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): GetMarketplaceDatasetsResponse {
      return {
        datasets: globalThis.Array.isArray(object?.datasets)
          ? object.datasets.map((e: any) => DatasetFile.fromJSON(e))
          : [],
      };
    },

    toJSON(message: GetMarketplaceDatasetsResponse): unknown {
      const obj: any = {};
      if (message.datasets?.length) {
        obj.datasets = message.datasets.map(e => DatasetFile.toJSON(e));
      }
      return obj;
    },

    create(
      base?: DeepPartial<GetMarketplaceDatasetsResponse>,
    ): GetMarketplaceDatasetsResponse {
      return GetMarketplaceDatasetsResponse.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<GetMarketplaceDatasetsResponse>,
    ): GetMarketplaceDatasetsResponse {
      const message = createBaseGetMarketplaceDatasetsResponse();
      message.datasets =
        object.datasets?.map(e => DatasetFile.fromPartial(e)) || [];
      return message;
    },
  };

function createBaseGetGravityTaskDatasetFilesRequest(): GetGravityTaskDatasetFilesRequest {
  return { gravityTaskId: "" };
}

export const GetGravityTaskDatasetFilesRequest: MessageFns<GetGravityTaskDatasetFilesRequest> =
  {
    encode(
      message: GetGravityTaskDatasetFilesRequest,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.gravityTaskId !== "") {
        writer.uint32(10).string(message.gravityTaskId);
      }
      return writer;
    },

    decode(
      input: BinaryReader | Uint8Array,
      length?: number,
    ): GetGravityTaskDatasetFilesRequest {
      const reader =
        input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseGetGravityTaskDatasetFilesRequest();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1: {
            if (tag !== 10) {
              break;
            }

            message.gravityTaskId = reader.string();
            continue;
          }
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): GetGravityTaskDatasetFilesRequest {
      return {
        gravityTaskId: isSet(object.gravityTaskId)
          ? globalThis.String(object.gravityTaskId)
          : "",
      };
    },

    toJSON(message: GetGravityTaskDatasetFilesRequest): unknown {
      const obj: any = {};
      if (message.gravityTaskId !== "") {
        obj.gravityTaskId = message.gravityTaskId;
      }
      return obj;
    },

    create(
      base?: DeepPartial<GetGravityTaskDatasetFilesRequest>,
    ): GetGravityTaskDatasetFilesRequest {
      return GetGravityTaskDatasetFilesRequest.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<GetGravityTaskDatasetFilesRequest>,
    ): GetGravityTaskDatasetFilesRequest {
      const message = createBaseGetGravityTaskDatasetFilesRequest();
      message.gravityTaskId = object.gravityTaskId ?? "";
      return message;
    },
  };

function createBaseCrawlerDatasetFiles(): CrawlerDatasetFiles {
  return { crawlerId: "", datasetFiles: [] };
}

export const CrawlerDatasetFiles: MessageFns<CrawlerDatasetFiles> = {
  encode(
    message: CrawlerDatasetFiles,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.crawlerId !== "") {
      writer.uint32(10).string(message.crawlerId);
    }
    for (const v of message.datasetFiles) {
      DatasetFileWithId.encode(v!, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(
    input: BinaryReader | Uint8Array,
    length?: number,
  ): CrawlerDatasetFiles {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseCrawlerDatasetFiles();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.crawlerId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.datasetFiles.push(
            DatasetFileWithId.decode(reader, reader.uint32()),
          );
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): CrawlerDatasetFiles {
    return {
      crawlerId: isSet(object.crawlerId)
        ? globalThis.String(object.crawlerId)
        : "",
      datasetFiles: globalThis.Array.isArray(object?.datasetFiles)
        ? object.datasetFiles.map((e: any) => DatasetFileWithId.fromJSON(e))
        : [],
    };
  },

  toJSON(message: CrawlerDatasetFiles): unknown {
    const obj: any = {};
    if (message.crawlerId !== "") {
      obj.crawlerId = message.crawlerId;
    }
    if (message.datasetFiles?.length) {
      obj.datasetFiles = message.datasetFiles.map(e =>
        DatasetFileWithId.toJSON(e),
      );
    }
    return obj;
  },

  create(base?: DeepPartial<CrawlerDatasetFiles>): CrawlerDatasetFiles {
    return CrawlerDatasetFiles.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<CrawlerDatasetFiles>): CrawlerDatasetFiles {
    const message = createBaseCrawlerDatasetFiles();
    message.crawlerId = object.crawlerId ?? "";
    message.datasetFiles =
      object.datasetFiles?.map(e => DatasetFileWithId.fromPartial(e)) || [];
    return message;
  },
};

function createBaseDatasetFileWithId(): DatasetFileWithId {
  return {
    datasetId: "",
    fileName: "",
    fileSizeBytes: 0,
    lastModified: undefined,
    numRows: 0,
    s3Key: "",
    url: "",
  };
}

export const DatasetFileWithId: MessageFns<DatasetFileWithId> = {
  encode(
    message: DatasetFileWithId,
    writer: BinaryWriter = new BinaryWriter(),
  ): BinaryWriter {
    if (message.datasetId !== "") {
      writer.uint32(10).string(message.datasetId);
    }
    if (message.fileName !== "") {
      writer.uint32(18).string(message.fileName);
    }
    if (message.fileSizeBytes !== 0) {
      writer.uint32(24).uint64(message.fileSizeBytes);
    }
    if (message.lastModified !== undefined) {
      Timestamp.encode(
        toTimestamp(message.lastModified),
        writer.uint32(34).fork(),
      ).join();
    }
    if (message.numRows !== 0) {
      writer.uint32(40).uint64(message.numRows);
    }
    if (message.s3Key !== "") {
      writer.uint32(50).string(message.s3Key);
    }
    if (message.url !== "") {
      writer.uint32(58).string(message.url);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): DatasetFileWithId {
    const reader =
      input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseDatasetFileWithId();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.datasetId = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.fileName = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.fileSizeBytes = longToNumber(reader.uint64());
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.lastModified = fromTimestamp(
            Timestamp.decode(reader, reader.uint32()),
          );
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.numRows = longToNumber(reader.uint64());
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.s3Key = reader.string();
          continue;
        }
        case 7: {
          if (tag !== 58) {
            break;
          }

          message.url = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): DatasetFileWithId {
    return {
      datasetId: isSet(object.datasetId)
        ? globalThis.String(object.datasetId)
        : "",
      fileName: isSet(object.fileName)
        ? globalThis.String(object.fileName)
        : "",
      fileSizeBytes: isSet(object.fileSizeBytes)
        ? globalThis.Number(object.fileSizeBytes)
        : 0,
      lastModified: isSet(object.lastModified)
        ? fromJsonTimestamp(object.lastModified)
        : undefined,
      numRows: isSet(object.numRows) ? globalThis.Number(object.numRows) : 0,
      s3Key: isSet(object.s3Key) ? globalThis.String(object.s3Key) : "",
      url: isSet(object.url) ? globalThis.String(object.url) : "",
    };
  },

  toJSON(message: DatasetFileWithId): unknown {
    const obj: any = {};
    if (message.datasetId !== "") {
      obj.datasetId = message.datasetId;
    }
    if (message.fileName !== "") {
      obj.fileName = message.fileName;
    }
    if (message.fileSizeBytes !== 0) {
      obj.fileSizeBytes = Math.round(message.fileSizeBytes);
    }
    if (message.lastModified !== undefined) {
      obj.lastModified = message.lastModified.toISOString();
    }
    if (message.numRows !== 0) {
      obj.numRows = Math.round(message.numRows);
    }
    if (message.s3Key !== "") {
      obj.s3Key = message.s3Key;
    }
    if (message.url !== "") {
      obj.url = message.url;
    }
    return obj;
  },

  create(base?: DeepPartial<DatasetFileWithId>): DatasetFileWithId {
    return DatasetFileWithId.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<DatasetFileWithId>): DatasetFileWithId {
    const message = createBaseDatasetFileWithId();
    message.datasetId = object.datasetId ?? "";
    message.fileName = object.fileName ?? "";
    message.fileSizeBytes = object.fileSizeBytes ?? 0;
    message.lastModified = object.lastModified ?? undefined;
    message.numRows = object.numRows ?? 0;
    message.s3Key = object.s3Key ?? "";
    message.url = object.url ?? "";
    return message;
  },
};

function createBaseGetGravityTaskDatasetFilesResponse(): GetGravityTaskDatasetFilesResponse {
  return { gravityTaskId: "", crawlerDatasetFiles: [] };
}

export const GetGravityTaskDatasetFilesResponse: MessageFns<GetGravityTaskDatasetFilesResponse> =
  {
    encode(
      message: GetGravityTaskDatasetFilesResponse,
      writer: BinaryWriter = new BinaryWriter(),
    ): BinaryWriter {
      if (message.gravityTaskId !== "") {
        writer.uint32(10).string(message.gravityTaskId);
      }
      for (const v of message.crawlerDatasetFiles) {
        CrawlerDatasetFiles.encode(v!, writer.uint32(18).fork()).join();
      }
      return writer;
    },

    decode(
      input: BinaryReader | Uint8Array,
      length?: number,
    ): GetGravityTaskDatasetFilesResponse {
      const reader =
        input instanceof BinaryReader ? input : new BinaryReader(input);
      let end = length === undefined ? reader.len : reader.pos + length;
      const message = createBaseGetGravityTaskDatasetFilesResponse();
      while (reader.pos < end) {
        const tag = reader.uint32();
        switch (tag >>> 3) {
          case 1: {
            if (tag !== 10) {
              break;
            }

            message.gravityTaskId = reader.string();
            continue;
          }
          case 2: {
            if (tag !== 18) {
              break;
            }

            message.crawlerDatasetFiles.push(
              CrawlerDatasetFiles.decode(reader, reader.uint32()),
            );
            continue;
          }
        }
        if ((tag & 7) === 4 || tag === 0) {
          break;
        }
        reader.skip(tag & 7);
      }
      return message;
    },

    fromJSON(object: any): GetGravityTaskDatasetFilesResponse {
      return {
        gravityTaskId: isSet(object.gravityTaskId)
          ? globalThis.String(object.gravityTaskId)
          : "",
        crawlerDatasetFiles: globalThis.Array.isArray(
          object?.crawlerDatasetFiles,
        )
          ? object.crawlerDatasetFiles.map((e: any) =>
              CrawlerDatasetFiles.fromJSON(e),
            )
          : [],
      };
    },

    toJSON(message: GetGravityTaskDatasetFilesResponse): unknown {
      const obj: any = {};
      if (message.gravityTaskId !== "") {
        obj.gravityTaskId = message.gravityTaskId;
      }
      if (message.crawlerDatasetFiles?.length) {
        obj.crawlerDatasetFiles = message.crawlerDatasetFiles.map(e =>
          CrawlerDatasetFiles.toJSON(e),
        );
      }
      return obj;
    },

    create(
      base?: DeepPartial<GetGravityTaskDatasetFilesResponse>,
    ): GetGravityTaskDatasetFilesResponse {
      return GetGravityTaskDatasetFilesResponse.fromPartial(base ?? {});
    },
    fromPartial(
      object: DeepPartial<GetGravityTaskDatasetFilesResponse>,
    ): GetGravityTaskDatasetFilesResponse {
      const message = createBaseGetGravityTaskDatasetFilesResponse();
      message.gravityTaskId = object.gravityTaskId ?? "";
      message.crawlerDatasetFiles =
        object.crawlerDatasetFiles?.map(e =>
          CrawlerDatasetFiles.fromPartial(e),
        ) || [];
      return message;
    },
  };

export type GravityServiceService = typeof GravityServiceService;
export const GravityServiceService = {
  /** Lists all data collection tasks for a user */
  getGravityTasks: {
    path: "/gravity.v1.GravityService/GetGravityTasks",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: GetGravityTasksRequest) =>
      Buffer.from(GetGravityTasksRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => GetGravityTasksRequest.decode(value),
    responseSerialize: (value: GetGravityTasksResponse) =>
      Buffer.from(GetGravityTasksResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) =>
      GetGravityTasksResponse.decode(value),
  },
  /** Get a single crawler by its ID */
  getCrawler: {
    path: "/gravity.v1.GravityService/GetCrawler",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: GetCrawlerRequest) =>
      Buffer.from(GetCrawlerRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => GetCrawlerRequest.decode(value),
    responseSerialize: (value: GetCrawlerResponse) =>
      Buffer.from(GetCrawlerResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => GetCrawlerResponse.decode(value),
  },
  /** Create a new gravity task */
  createGravityTask: {
    path: "/gravity.v1.GravityService/CreateGravityTask",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: CreateGravityTaskRequest) =>
      Buffer.from(CreateGravityTaskRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) =>
      CreateGravityTaskRequest.decode(value),
    responseSerialize: (value: CreateGravityTaskResponse) =>
      Buffer.from(CreateGravityTaskResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) =>
      CreateGravityTaskResponse.decode(value),
  },
  /** Build a dataset for a single crawler */
  buildDataset: {
    path: "/gravity.v1.GravityService/BuildDataset",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: BuildDatasetRequest) =>
      Buffer.from(BuildDatasetRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => BuildDatasetRequest.decode(value),
    responseSerialize: (value: BuildDatasetResponse) =>
      Buffer.from(BuildDatasetResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => BuildDatasetResponse.decode(value),
  },
  /** Get the dataset build status and results */
  getDataset: {
    path: "/gravity.v1.GravityService/GetDataset",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: GetDatasetRequest) =>
      Buffer.from(GetDatasetRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => GetDatasetRequest.decode(value),
    responseSerialize: (value: GetDatasetResponse) =>
      Buffer.from(GetDatasetResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => GetDatasetResponse.decode(value),
  },
  /** Cancel a gravity task and any crawlers associated with it */
  cancelGravityTask: {
    path: "/gravity.v1.GravityService/CancelGravityTask",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: CancelGravityTaskRequest) =>
      Buffer.from(CancelGravityTaskRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) =>
      CancelGravityTaskRequest.decode(value),
    responseSerialize: (value: CancelGravityTaskResponse) =>
      Buffer.from(CancelGravityTaskResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) =>
      CancelGravityTaskResponse.decode(value),
  },
  /** Cancel dataset build if it is in progress and purges the dataset */
  cancelDataset: {
    path: "/gravity.v1.GravityService/CancelDataset",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: CancelDatasetRequest) =>
      Buffer.from(CancelDatasetRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => CancelDatasetRequest.decode(value),
    responseSerialize: (value: CancelDatasetResponse) =>
      Buffer.from(CancelDatasetResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => CancelDatasetResponse.decode(value),
  },
  /** Refund user if fewer rows are returned */
  datasetBillingCorrection: {
    path: "/gravity.v1.GravityService/DatasetBillingCorrection",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: DatasetBillingCorrectionRequest) =>
      Buffer.from(DatasetBillingCorrectionRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) =>
      DatasetBillingCorrectionRequest.decode(value),
    responseSerialize: (value: DatasetBillingCorrectionResponse) =>
      Buffer.from(DatasetBillingCorrectionResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) =>
      DatasetBillingCorrectionResponse.decode(value),
  },
  /** Gets the available datsets for use in Dataset Marketplace */
  getMarketplaceDatasets: {
    path: "/gravity.v1.GravityService/GetMarketplaceDatasets",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: Empty) =>
      Buffer.from(Empty.encode(value).finish()),
    requestDeserialize: (value: Buffer) => Empty.decode(value),
    responseSerialize: (value: GetMarketplaceDatasetsResponse) =>
      Buffer.from(GetMarketplaceDatasetsResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) =>
      GetMarketplaceDatasetsResponse.decode(value),
  },
  /** Gets all dataset files for a given gravity task */
  getGravityTaskDatasetFiles: {
    path: "/gravity.v1.GravityService/GetGravityTaskDatasetFiles",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: GetGravityTaskDatasetFilesRequest) =>
      Buffer.from(GetGravityTaskDatasetFilesRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) =>
      GetGravityTaskDatasetFilesRequest.decode(value),
    responseSerialize: (value: GetGravityTaskDatasetFilesResponse) =>
      Buffer.from(GetGravityTaskDatasetFilesResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) =>
      GetGravityTaskDatasetFilesResponse.decode(value),
  },
  /** Publishes a dataset into the Marketplace */
  publishDataset: {
    path: "/gravity.v1.GravityService/PublishDataset",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: PublishDatasetRequest) =>
      Buffer.from(PublishDatasetRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => PublishDatasetRequest.decode(value),
    responseSerialize: (value: UpsertResponse) =>
      Buffer.from(UpsertResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => UpsertResponse.decode(value),
  },
  /** Upserts a crawler into the Gravity state DB */
  upsertCrawler: {
    path: "/gravity.v1.GravityService/UpsertCrawler",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: UpsertCrawlerRequest) =>
      Buffer.from(UpsertCrawlerRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => UpsertCrawlerRequest.decode(value),
    responseSerialize: (value: UpsertResponse) =>
      Buffer.from(UpsertResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => UpsertResponse.decode(value),
  },
  /** Upserts a crawler criteria into the Gravity state DB */
  insertCrawlerCriteria: {
    path: "/gravity.v1.GravityService/InsertCrawlerCriteria",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: InsertCrawlerCriteriaRequest) =>
      Buffer.from(InsertCrawlerCriteriaRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) =>
      InsertCrawlerCriteriaRequest.decode(value),
    responseSerialize: (value: UpsertResponse) =>
      Buffer.from(UpsertResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => UpsertResponse.decode(value),
  },
  /** Upserts a gravity task into the Gravity state DB */
  upsertGravityTask: {
    path: "/gravity.v1.GravityService/UpsertGravityTask",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: UpsertGravityTaskRequest) =>
      Buffer.from(UpsertGravityTaskRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) =>
      UpsertGravityTaskRequest.decode(value),
    responseSerialize: (value: UpsertGravityTaskResponse) =>
      Buffer.from(UpsertGravityTaskResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) =>
      UpsertGravityTaskResponse.decode(value),
  },
  /** Upserts a dataset into to the Gravity state DB */
  upsertDataset: {
    path: "/gravity.v1.GravityService/UpsertDataset",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: UpsertDatasetRequest) =>
      Buffer.from(UpsertDatasetRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => UpsertDatasetRequest.decode(value),
    responseSerialize: (value: UpsertResponse) =>
      Buffer.from(UpsertResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => UpsertResponse.decode(value),
  },
  /** Inserts a dataset file row into the Gravity state DB */
  insertDatasetFile: {
    path: "/gravity.v1.GravityService/InsertDatasetFile",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: InsertDatasetFileRequest) =>
      Buffer.from(InsertDatasetFileRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) =>
      InsertDatasetFileRequest.decode(value),
    responseSerialize: (value: UpsertResponse) =>
      Buffer.from(UpsertResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => UpsertResponse.decode(value),
  },
  /** Upserts a nebula into the Gravity nebula DB */
  upsertNebula: {
    path: "/gravity.v1.GravityService/UpsertNebula",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: UpsertNebulaRequest) =>
      Buffer.from(UpsertNebulaRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) => UpsertNebulaRequest.decode(value),
    responseSerialize: (value: UpsertResponse) =>
      Buffer.from(UpsertResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) => UpsertResponse.decode(value),
  },
  /** Builds all datasets for a task (additionally cancels crawlers with no data) */
  buildAllDatasets: {
    path: "/gravity.v1.GravityService/BuildAllDatasets",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: BuildAllDatasetsRequest) =>
      Buffer.from(BuildAllDatasetsRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer) =>
      BuildAllDatasetsRequest.decode(value),
    responseSerialize: (value: BuildAllDatasetsResponse) =>
      Buffer.from(BuildAllDatasetsResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer) =>
      BuildAllDatasetsResponse.decode(value),
  },
} as const;

export interface GravityServiceServer extends UntypedServiceImplementation {
  /** Lists all data collection tasks for a user */
  getGravityTasks: handleUnaryCall<
    GetGravityTasksRequest,
    GetGravityTasksResponse
  >;
  /** Get a single crawler by its ID */
  getCrawler: handleUnaryCall<GetCrawlerRequest, GetCrawlerResponse>;
  /** Create a new gravity task */
  createGravityTask: handleUnaryCall<
    CreateGravityTaskRequest,
    CreateGravityTaskResponse
  >;
  /** Build a dataset for a single crawler */
  buildDataset: handleUnaryCall<BuildDatasetRequest, BuildDatasetResponse>;
  /** Get the dataset build status and results */
  getDataset: handleUnaryCall<GetDatasetRequest, GetDatasetResponse>;
  /** Cancel a gravity task and any crawlers associated with it */
  cancelGravityTask: handleUnaryCall<
    CancelGravityTaskRequest,
    CancelGravityTaskResponse
  >;
  /** Cancel dataset build if it is in progress and purges the dataset */
  cancelDataset: handleUnaryCall<CancelDatasetRequest, CancelDatasetResponse>;
  /** Refund user if fewer rows are returned */
  datasetBillingCorrection: handleUnaryCall<
    DatasetBillingCorrectionRequest,
    DatasetBillingCorrectionResponse
  >;
  /** Gets the available datsets for use in Dataset Marketplace */
  getMarketplaceDatasets: handleUnaryCall<
    Empty,
    GetMarketplaceDatasetsResponse
  >;
  /** Gets all dataset files for a given gravity task */
  getGravityTaskDatasetFiles: handleUnaryCall<
    GetGravityTaskDatasetFilesRequest,
    GetGravityTaskDatasetFilesResponse
  >;
  /** Publishes a dataset into the Marketplace */
  publishDataset: handleUnaryCall<PublishDatasetRequest, UpsertResponse>;
  /** Upserts a crawler into the Gravity state DB */
  upsertCrawler: handleUnaryCall<UpsertCrawlerRequest, UpsertResponse>;
  /** Upserts a crawler criteria into the Gravity state DB */
  insertCrawlerCriteria: handleUnaryCall<
    InsertCrawlerCriteriaRequest,
    UpsertResponse
  >;
  /** Upserts a gravity task into the Gravity state DB */
  upsertGravityTask: handleUnaryCall<
    UpsertGravityTaskRequest,
    UpsertGravityTaskResponse
  >;
  /** Upserts a dataset into to the Gravity state DB */
  upsertDataset: handleUnaryCall<UpsertDatasetRequest, UpsertResponse>;
  /** Inserts a dataset file row into the Gravity state DB */
  insertDatasetFile: handleUnaryCall<InsertDatasetFileRequest, UpsertResponse>;
  /** Upserts a nebula into the Gravity nebula DB */
  upsertNebula: handleUnaryCall<UpsertNebulaRequest, UpsertResponse>;
  /** Builds all datasets for a task (additionally cancels crawlers with no data) */
  buildAllDatasets: handleUnaryCall<
    BuildAllDatasetsRequest,
    BuildAllDatasetsResponse
  >;
}

export interface GravityServiceClient extends Client {
  /** Lists all data collection tasks for a user */
  getGravityTasks(
    request: GetGravityTasksRequest,
    callback: (
      error: ServiceError | null,
      response: GetGravityTasksResponse,
    ) => void,
  ): ClientUnaryCall;
  getGravityTasks(
    request: GetGravityTasksRequest,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: GetGravityTasksResponse,
    ) => void,
  ): ClientUnaryCall;
  getGravityTasks(
    request: GetGravityTasksRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: GetGravityTasksResponse,
    ) => void,
  ): ClientUnaryCall;
  /** Get a single crawler by its ID */
  getCrawler(
    request: GetCrawlerRequest,
    callback: (
      error: ServiceError | null,
      response: GetCrawlerResponse,
    ) => void,
  ): ClientUnaryCall;
  getCrawler(
    request: GetCrawlerRequest,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: GetCrawlerResponse,
    ) => void,
  ): ClientUnaryCall;
  getCrawler(
    request: GetCrawlerRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: GetCrawlerResponse,
    ) => void,
  ): ClientUnaryCall;
  /** Create a new gravity task */
  createGravityTask(
    request: CreateGravityTaskRequest,
    callback: (
      error: ServiceError | null,
      response: CreateGravityTaskResponse,
    ) => void,
  ): ClientUnaryCall;
  createGravityTask(
    request: CreateGravityTaskRequest,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: CreateGravityTaskResponse,
    ) => void,
  ): ClientUnaryCall;
  createGravityTask(
    request: CreateGravityTaskRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: CreateGravityTaskResponse,
    ) => void,
  ): ClientUnaryCall;
  /** Build a dataset for a single crawler */
  buildDataset(
    request: BuildDatasetRequest,
    callback: (
      error: ServiceError | null,
      response: BuildDatasetResponse,
    ) => void,
  ): ClientUnaryCall;
  buildDataset(
    request: BuildDatasetRequest,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: BuildDatasetResponse,
    ) => void,
  ): ClientUnaryCall;
  buildDataset(
    request: BuildDatasetRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: BuildDatasetResponse,
    ) => void,
  ): ClientUnaryCall;
  /** Get the dataset build status and results */
  getDataset(
    request: GetDatasetRequest,
    callback: (
      error: ServiceError | null,
      response: GetDatasetResponse,
    ) => void,
  ): ClientUnaryCall;
  getDataset(
    request: GetDatasetRequest,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: GetDatasetResponse,
    ) => void,
  ): ClientUnaryCall;
  getDataset(
    request: GetDatasetRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: GetDatasetResponse,
    ) => void,
  ): ClientUnaryCall;
  /** Cancel a gravity task and any crawlers associated with it */
  cancelGravityTask(
    request: CancelGravityTaskRequest,
    callback: (
      error: ServiceError | null,
      response: CancelGravityTaskResponse,
    ) => void,
  ): ClientUnaryCall;
  cancelGravityTask(
    request: CancelGravityTaskRequest,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: CancelGravityTaskResponse,
    ) => void,
  ): ClientUnaryCall;
  cancelGravityTask(
    request: CancelGravityTaskRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: CancelGravityTaskResponse,
    ) => void,
  ): ClientUnaryCall;
  /** Cancel dataset build if it is in progress and purges the dataset */
  cancelDataset(
    request: CancelDatasetRequest,
    callback: (
      error: ServiceError | null,
      response: CancelDatasetResponse,
    ) => void,
  ): ClientUnaryCall;
  cancelDataset(
    request: CancelDatasetRequest,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: CancelDatasetResponse,
    ) => void,
  ): ClientUnaryCall;
  cancelDataset(
    request: CancelDatasetRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: CancelDatasetResponse,
    ) => void,
  ): ClientUnaryCall;
  /** Refund user if fewer rows are returned */
  datasetBillingCorrection(
    request: DatasetBillingCorrectionRequest,
    callback: (
      error: ServiceError | null,
      response: DatasetBillingCorrectionResponse,
    ) => void,
  ): ClientUnaryCall;
  datasetBillingCorrection(
    request: DatasetBillingCorrectionRequest,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: DatasetBillingCorrectionResponse,
    ) => void,
  ): ClientUnaryCall;
  datasetBillingCorrection(
    request: DatasetBillingCorrectionRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: DatasetBillingCorrectionResponse,
    ) => void,
  ): ClientUnaryCall;
  /** Gets the available datsets for use in Dataset Marketplace */
  getMarketplaceDatasets(
    request: Empty,
    callback: (
      error: ServiceError | null,
      response: GetMarketplaceDatasetsResponse,
    ) => void,
  ): ClientUnaryCall;
  getMarketplaceDatasets(
    request: Empty,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: GetMarketplaceDatasetsResponse,
    ) => void,
  ): ClientUnaryCall;
  getMarketplaceDatasets(
    request: Empty,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: GetMarketplaceDatasetsResponse,
    ) => void,
  ): ClientUnaryCall;
  /** Gets all dataset files for a given gravity task */
  getGravityTaskDatasetFiles(
    request: GetGravityTaskDatasetFilesRequest,
    callback: (
      error: ServiceError | null,
      response: GetGravityTaskDatasetFilesResponse,
    ) => void,
  ): ClientUnaryCall;
  getGravityTaskDatasetFiles(
    request: GetGravityTaskDatasetFilesRequest,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: GetGravityTaskDatasetFilesResponse,
    ) => void,
  ): ClientUnaryCall;
  getGravityTaskDatasetFiles(
    request: GetGravityTaskDatasetFilesRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: GetGravityTaskDatasetFilesResponse,
    ) => void,
  ): ClientUnaryCall;
  /** Publishes a dataset into the Marketplace */
  publishDataset(
    request: PublishDatasetRequest,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  publishDataset(
    request: PublishDatasetRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  publishDataset(
    request: PublishDatasetRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  /** Upserts a crawler into the Gravity state DB */
  upsertCrawler(
    request: UpsertCrawlerRequest,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  upsertCrawler(
    request: UpsertCrawlerRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  upsertCrawler(
    request: UpsertCrawlerRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  /** Upserts a crawler criteria into the Gravity state DB */
  insertCrawlerCriteria(
    request: InsertCrawlerCriteriaRequest,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  insertCrawlerCriteria(
    request: InsertCrawlerCriteriaRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  insertCrawlerCriteria(
    request: InsertCrawlerCriteriaRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  /** Upserts a gravity task into the Gravity state DB */
  upsertGravityTask(
    request: UpsertGravityTaskRequest,
    callback: (
      error: ServiceError | null,
      response: UpsertGravityTaskResponse,
    ) => void,
  ): ClientUnaryCall;
  upsertGravityTask(
    request: UpsertGravityTaskRequest,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: UpsertGravityTaskResponse,
    ) => void,
  ): ClientUnaryCall;
  upsertGravityTask(
    request: UpsertGravityTaskRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: UpsertGravityTaskResponse,
    ) => void,
  ): ClientUnaryCall;
  /** Upserts a dataset into to the Gravity state DB */
  upsertDataset(
    request: UpsertDatasetRequest,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  upsertDataset(
    request: UpsertDatasetRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  upsertDataset(
    request: UpsertDatasetRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  /** Inserts a dataset file row into the Gravity state DB */
  insertDatasetFile(
    request: InsertDatasetFileRequest,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  insertDatasetFile(
    request: InsertDatasetFileRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  insertDatasetFile(
    request: InsertDatasetFileRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  /** Upserts a nebula into the Gravity nebula DB */
  upsertNebula(
    request: UpsertNebulaRequest,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  upsertNebula(
    request: UpsertNebulaRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  upsertNebula(
    request: UpsertNebulaRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: UpsertResponse) => void,
  ): ClientUnaryCall;
  /** Builds all datasets for a task (additionally cancels crawlers with no data) */
  buildAllDatasets(
    request: BuildAllDatasetsRequest,
    callback: (
      error: ServiceError | null,
      response: BuildAllDatasetsResponse,
    ) => void,
  ): ClientUnaryCall;
  buildAllDatasets(
    request: BuildAllDatasetsRequest,
    metadata: Metadata,
    callback: (
      error: ServiceError | null,
      response: BuildAllDatasetsResponse,
    ) => void,
  ): ClientUnaryCall;
  buildAllDatasets(
    request: BuildAllDatasetsRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (
      error: ServiceError | null,
      response: BuildAllDatasetsResponse,
    ) => void,
  ): ClientUnaryCall;
}

export const GravityServiceClient = makeGenericClientConstructor(
  GravityServiceService,
  "gravity.v1.GravityService",
) as unknown as {
  new (
    address: string,
    credentials: ChannelCredentials,
    options?: Partial<ClientOptions>,
  ): GravityServiceClient;
  service: typeof GravityServiceService;
  serviceName: string;
};

type Builtin =
  | Date
  | Function
  | Uint8Array
  | string
  | number
  | boolean
  | undefined;

export type DeepPartial<T> = T extends Builtin
  ? T
  : T extends globalThis.Array<infer U>
    ? globalThis.Array<DeepPartial<U>>
    : T extends ReadonlyArray<infer U>
      ? ReadonlyArray<DeepPartial<U>>
      : T extends {}
        ? { [K in keyof T]?: DeepPartial<T[K]> }
        : Partial<T>;

function toTimestamp(date: Date): Timestamp {
  const seconds = Math.trunc(date.getTime() / 1_000);
  const nanos = (date.getTime() % 1_000) * 1_000_000;
  return { seconds, nanos };
}

function fromTimestamp(t: Timestamp): Date {
  let millis = (t.seconds || 0) * 1_000;
  millis += (t.nanos || 0) / 1_000_000;
  return new globalThis.Date(millis);
}

function fromJsonTimestamp(o: any): Date {
  if (o instanceof globalThis.Date) {
    return o;
  } else if (typeof o === "string") {
    return new globalThis.Date(o);
  } else {
    return fromTimestamp(Timestamp.fromJSON(o));
  }
}

function longToNumber(int64: { toString(): string }): number {
  const num = globalThis.Number(int64.toString());
  if (num > globalThis.Number.MAX_SAFE_INTEGER) {
    throw new globalThis.Error("Value is larger than Number.MAX_SAFE_INTEGER");
  }
  if (num < globalThis.Number.MIN_SAFE_INTEGER) {
    throw new globalThis.Error("Value is smaller than Number.MIN_SAFE_INTEGER");
  }
  return num;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
